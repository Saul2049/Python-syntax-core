"""
CSVæ•°æ®åŠ è½½å™¨æ¨¡å— (CSV Data Loader Module)

æä¾›CSVæ–‡ä»¶æ•°æ®åŠ è½½åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- åŸºç¡€CSVåŠ è½½
- OHLCVæ•°æ®å¤„ç†
- æ•°æ®éªŒè¯
- æ ¼å¼åŒ–å¤„ç†
"""

import os
from pathlib import Path
from typing import List, Optional, Union

import pandas as pd


class CSVDataLoader:
    """CSVæ•°æ®åŠ è½½å™¨ç±»"""

    def __init__(self, base_path: Optional[Union[str, Path]] = None):
        """
        åˆå§‹åŒ–CSVæ•°æ®åŠ è½½å™¨

        å‚æ•°:
            base_path: æ•°æ®æ–‡ä»¶åŸºç¡€è·¯å¾„
        """
        self.base_path = Path(base_path) if base_path else Path.cwd()

    def load_data(
        self, file_path: Union[str, Path], columns: Optional[List[str]] = None, **kwargs
    ) -> pd.DataFrame:
        """
        ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®

        å‚æ•°:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            columns: è¦åŠ è½½çš„åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            **kwargs: pandas.read_csvçš„å…¶ä»–å‚æ•°

        è¿”å›:
            pandas DataFrame
        """
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„ä½ç½®
        possible_paths = [
            file_path,  # åŸå§‹è·¯å¾„
            self.base_path / file_path,  # åŸºç¡€è·¯å¾„ + æ–‡ä»¶è·¯å¾„
            Path.cwd() / file_path,  # å½“å‰å·¥ä½œç›®å½•
            Path(__file__).parent.parent.parent / file_path,  # é¡¹ç›®æ ¹ç›®å½•
        ]

        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ·»åŠ æ›´å¤šå¯èƒ½çš„ä½ç½®
        if not Path(file_path).is_absolute():
            possible_paths.extend(
                [
                    Path(__file__).parent.parent.parent.parent / file_path,  # ä¸Šçº§ç›®å½•
                    Path.cwd().parent / file_path,  # çˆ¶ç›®å½•
                ]
            )

        for path in possible_paths:
            try:
                abs_path = Path(path).resolve()
                if abs_path.exists():
                    if columns:
                        df = pd.read_csv(abs_path, usecols=columns, **kwargs)
                    else:
                        df = pd.read_csv(abs_path, **kwargs)

                    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {abs_path} ({len(df)} è¡Œ)")
                    return df
            except (FileNotFoundError, pd.errors.EmptyDataError, pd.errors.ParserError):
                continue
            except Exception as e:
                print(f"âš ï¸ å°è¯•åŠ è½½ {path} æ—¶å‡ºé”™: {str(e)}")
                continue

        # å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥ï¼Œæ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"âŒ é”™è¯¯: åœ¨ä»¥ä¸‹è·¯å¾„ä¸­éƒ½æœªæ‰¾åˆ°æ–‡ä»¶ '{file_path}':")
        for path in possible_paths:
            abs_path = Path(path).resolve()
            print(f"  - {abs_path}")

        # å¯¹äºç‰¹å®šçš„æ–‡ä»¶åï¼Œç”Ÿæˆfallbackæ•°æ®
        if str(file_path) == "btc_eth.csv" or Path(file_path).name == "btc_eth.csv":
            print("ğŸ”„ ä½¿ç”¨åˆæˆæ•°æ®ä½œä¸ºfallback...")
            return self._generate_fallback_data(**kwargs)

        print("âŒ æ— æ³•ç”Ÿæˆfallbackæ•°æ®ï¼Œè¿”å›ç©ºDataFrame")
        return pd.DataFrame()

    def _generate_fallback_data(self, **kwargs) -> pd.DataFrame:
        """
        ç”Ÿæˆfallbackåˆæˆæ•°æ®

        å‚æ•°:
            **kwargs: pandaså‚æ•°ï¼Œç”¨äºç¡®å®šæ˜¯å¦éœ€è¦æ—¥æœŸç´¢å¼•ç­‰

        è¿”å›:
            åˆæˆçš„DataFrame
        """
        from datetime import datetime, timedelta

        import numpy as np

        # ç”Ÿæˆæ—¥æœŸèŒƒå›´
        days = 1000
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        dates = pd.date_range(start=start_date, end=end_date, freq="D")

        # ç”Ÿæˆåˆæˆä»·æ ¼æ•°æ®
        np.random.seed(42)  # ç¡®ä¿å¯é‡ç°

        # BTCä»·æ ¼æ•°æ®
        btc_returns = np.random.normal(0.001, 0.03, len(dates))
        btc_prices = [50000]
        for ret in btc_returns[1:]:
            btc_prices.append(btc_prices[-1] * (1 + ret))

        # ETHä»·æ ¼æ•°æ®
        eth_returns = np.random.normal(0.0015, 0.04, len(dates))
        eth_prices = [3000]
        for ret in eth_returns[1:]:
            eth_prices.append(eth_prices[-1] * (1 + ret))

        # åˆ›å»ºDataFrame
        df = pd.DataFrame({"date": dates, "btc": btc_prices, "eth": eth_prices})

        # æ ¹æ®kwargså¤„ç†æ—¥æœŸç´¢å¼•
        if kwargs.get("parse_dates") and "date" in kwargs.get("parse_dates", []):
            df["date"] = pd.to_datetime(df["date"])

        if kwargs.get("index_col") == "date":
            df.set_index("date", inplace=True)

        print(
            f"âœ… ç”Ÿæˆäº† {len(df)} è¡Œåˆæˆæ•°æ® (BTC: {df['btc'].iloc[0]:.2f} -> {df['btc'].iloc[-1]:.2f})"
        )
        return df

    def load_ohlcv_data(
        self, file_path: Union[str, Path], date_column: str = "date", validate_columns: bool = True
    ) -> pd.DataFrame:
        """
        åŠ è½½OHLCVæ ¼å¼çš„äº¤æ˜“æ•°æ®

        å‚æ•°:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            date_column: æ—¥æœŸåˆ—å
            validate_columns: æ˜¯å¦éªŒè¯å¿…éœ€åˆ—çš„å­˜åœ¨

        è¿”å›:
            å¤„ç†åçš„OHLCV DataFrame
        """
        df = self.load_data(file_path)

        if df.empty:
            return df

        # éªŒè¯å¿…éœ€çš„OHLCVåˆ—
        if validate_columns:
            required_columns = ["open", "high", "low", "close"]
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                print(f"âš ï¸ è­¦å‘Š: ç¼ºå¤±å¿…éœ€åˆ—: {missing_columns}")
                # å°è¯•æŸ¥æ‰¾ç›¸ä¼¼åˆ—å
                self._suggest_column_mapping(df.columns, missing_columns)

        return self._process_ohlcv_data(df, date_column)

    def _process_ohlcv_data(self, df: pd.DataFrame, date_column: str = "date") -> pd.DataFrame:
        """
        å¤„ç†OHLCVæ•°æ®çš„å†…éƒ¨æ–¹æ³•

        å‚æ•°:
            df: åŸå§‹DataFrame
            date_column: æ—¥æœŸåˆ—å

        è¿”å›:
            å¤„ç†åçš„DataFrame
        """
        # å¤åˆ¶DataFrameä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df_processed = df.copy()

        # å¤„ç†æ—¥æœŸåˆ—
        if date_column in df_processed.columns:
            df_processed[date_column] = pd.to_datetime(df_processed[date_column])
            df_processed.set_index(date_column, inplace=True)
            df_processed.sort_index(inplace=True)
        else:
            print(f"âš ï¸ è­¦å‘Š: æ—¥æœŸåˆ— '{date_column}' ä¸å­˜åœ¨")

        # æ•°æ®ç±»å‹è½¬æ¢
        numeric_columns = ["open", "high", "low", "close", "volume"]
        for col in numeric_columns:
            if col in df_processed.columns:
                df_processed[col] = pd.to_numeric(df_processed[col], errors="coerce")

        return df_processed

    def _suggest_column_mapping(self, existing_columns: List[str], missing_columns: List[str]):
        """
        å»ºè®®åˆ—åæ˜ å°„

        å‚æ•°:
            existing_columns: ç°æœ‰åˆ—å
            missing_columns: ç¼ºå¤±åˆ—å
        """
        suggestions = {
            "open": ["Open", "OPEN", "o", "opening"],
            "high": ["High", "HIGH", "h", "max"],
            "low": ["Low", "LOW", "l", "min"],
            "close": ["Close", "CLOSE", "c", "closing"],
            "volume": ["Volume", "VOLUME", "v", "vol"],
        }

        for missing_col in missing_columns:
            if missing_col in suggestions:
                possible_matches = [
                    col for col in existing_columns if col in suggestions[missing_col]
                ]
                if possible_matches:
                    print(f"  ğŸ’¡ å»ºè®®: '{missing_col}' å¯èƒ½å¯¹åº” {possible_matches}")

    def load_multiple_files(
        self, file_patterns: List[Union[str, Path]], combine: bool = False
    ) -> Union[List[pd.DataFrame], pd.DataFrame]:
        """
        åŠ è½½å¤šä¸ªCSVæ–‡ä»¶

        å‚æ•°:
            file_patterns: æ–‡ä»¶è·¯å¾„æ¨¡å¼åˆ—è¡¨
            combine: æ˜¯å¦åˆå¹¶æ‰€æœ‰æ•°æ®

        è¿”å›:
            DataFrameåˆ—è¡¨æˆ–åˆå¹¶åçš„DataFrame
        """
        dataframes = []

        for pattern in file_patterns:
            if isinstance(pattern, str) and ("*" in pattern or "?" in pattern):
                # å¤„ç†é€šé…ç¬¦æ¨¡å¼
                matching_files = list(self.base_path.glob(pattern))
            else:
                matching_files = [self.base_path / pattern]

            for file_path in matching_files:
                if file_path.exists():
                    df = self.load_data(file_path)
                    if not df.empty:
                        df["source_file"] = file_path.name  # æ·»åŠ æ¥æºæ–‡ä»¶ä¿¡æ¯
                        dataframes.append(df)

        if combine and dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            print(f"âœ… åˆå¹¶äº† {len(dataframes)} ä¸ªæ–‡ä»¶çš„æ•°æ®")
            return combined_df

        return dataframes

    def get_file_info(self, file_path: Union[str, Path]) -> dict:
        """
        è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„

        è¿”å›:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        if not Path(file_path).is_absolute():
            file_path = self.base_path / file_path

        try:
            file_stats = os.stat(file_path)
            # è¯»å–æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
            sample_df = pd.read_csv(file_path, nrows=5)

            return {
                "file_path": str(file_path),
                "file_size_mb": round(file_stats.st_size / (1024 * 1024), 2),
                "columns": list(sample_df.columns),
                "column_count": len(sample_df.columns),
                "sample_rows": len(sample_df),
                "exists": True,
            }
        except Exception as e:
            return {"file_path": str(file_path), "error": str(e), "exists": False}


# ä¾¿æ·å‡½æ•°
def load_csv(
    file_path: Union[str, Path] = "btc_eth.csv",  # æ·»åŠ é»˜è®¤å€¼ä»¥ä¿æŒå‘åå…¼å®¹
    columns: Optional[List[str]] = None,
    base_path: Optional[Union[str, Path]] = None,
    **kwargs,
) -> pd.DataFrame:
    """
    ä¾¿æ·çš„CSVåŠ è½½å‡½æ•°

    å‚æ•°:
        file_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤btc_eth.csvä»¥ä¿æŒå‘åå…¼å®¹ï¼‰
        columns: è¦åŠ è½½çš„åˆ—ååˆ—è¡¨
        base_path: åŸºç¡€è·¯å¾„
        **kwargs: pandas.read_csvçš„å…¶ä»–å‚æ•°

    è¿”å›:
        pandas DataFrame
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šç‰¹æ®Šå‚æ•°ï¼Œä½¿ç”¨æ—§çš„è¡Œä¸ºï¼ˆè§£ææ—¥æœŸå’Œè®¾ç½®ç´¢å¼•ï¼‰
    if not kwargs and file_path == "btc_eth.csv":
        kwargs = {"parse_dates": ["date"], "index_col": "date"}

    loader = CSVDataLoader(base_path)
    return loader.load_data(file_path, columns, **kwargs)


def load_ohlcv_csv(
    file_path: Union[str, Path],
    date_column: str = "date",
    base_path: Optional[Union[str, Path]] = None,
) -> pd.DataFrame:
    """
    ä¾¿æ·çš„OHLCV CSVåŠ è½½å‡½æ•°

    å‚æ•°:
        file_path: CSVæ–‡ä»¶è·¯å¾„
        date_column: æ—¥æœŸåˆ—å
        base_path: åŸºç¡€è·¯å¾„

    è¿”å›:
        å¤„ç†åçš„OHLCV DataFrame
    """
    loader = CSVDataLoader(base_path)
    return loader.load_ohlcv_data(file_path, date_column)
