"""
æ•°æ®ä¿å­˜å™¨æ¨¡å— (Data Saver Module)

æä¾›æ•°æ®ä¿å­˜å’Œå¯¼å‡ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¤šæ ¼å¼æ•°æ®ä¿å­˜
- æ•°æ®éªŒè¯
- æ‰¹é‡å¤„ç†
- å¤‡ä»½ç®¡ç†
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import pandas as pd


class DataSaver:
    """æ•°æ®ä¿å­˜å™¨ç±»"""

    def __init__(self, base_output_dir: Optional[Union[str, Path]] = None):
        """
        åˆå§‹åŒ–æ•°æ®ä¿å­˜å™¨

        å‚æ•°:
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
        """
        self.base_output_dir = Path(base_output_dir) if base_output_dir else Path("output")
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

    def save_data(
        self,
        df: pd.DataFrame,
        filename: str,
        file_format: str = "csv",
        subdirectory: Optional[str] = None,
        create_backup: bool = False,
        **kwargs,
    ) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶

        å‚æ•°:
            df: è¦ä¿å­˜çš„DataFrame
            filename: æ–‡ä»¶å
            file_format: æ–‡ä»¶æ ¼å¼ ('csv', 'parquet', 'pickle', 'json', 'excel')
            subdirectory: å­ç›®å½•å
            create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            **kwargs: ä¿å­˜æ–¹æ³•çš„é¢å¤–å‚æ•°

        è¿”å›:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ„å»ºå®Œæ•´è·¯å¾„
            if subdirectory:
                output_dir = self.base_output_dir / subdirectory
                output_dir.mkdir(parents=True, exist_ok=True)
            else:
                output_dir = self.base_output_dir

            # æå–ç‰¹æ®Šå‚æ•°
            add_timestamp = kwargs.pop("add_timestamp", False)
            save_metadata = kwargs.pop("save_metadata", True)

            # æ·»åŠ æ—¶é—´æˆ³åˆ°æ–‡ä»¶å(å¦‚æœéœ€è¦)
            if add_timestamp:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                name, ext = os.path.splitext(filename)
                filename = f"{name}_{timestamp}{ext}"

            file_path = output_dir / filename

            # åˆ›å»ºå¤‡ä»½
            if create_backup and file_path.exists():
                self._create_backup(file_path)

            # æ ¹æ®æ ¼å¼ä¿å­˜æ•°æ®
            success = self._save_by_format(df, file_path, file_format, **kwargs)

            if success:
                print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {file_path}")

                # ä¿å­˜å…ƒæ•°æ®
                if save_metadata:
                    self._save_metadata(df, file_path)

                return True
            else:
                return False

        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False

    def _save_by_format(
        self, df: pd.DataFrame, file_path: Path, file_format: str, **kwargs
    ) -> bool:
        """
        æ ¹æ®æ ¼å¼ä¿å­˜æ•°æ®

        å‚æ•°:
            df: DataFrame
            file_path: æ–‡ä»¶è·¯å¾„
            file_format: æ–‡ä»¶æ ¼å¼
            **kwargs: é¢å¤–å‚æ•°

        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            format_lower = file_format.lower()

            if not self._is_supported_format(format_lower):
                print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
                return False

            self._execute_save_operation(df, file_path, format_lower, **kwargs)
            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜æ ¼å¼ {file_format} æ—¶å‡ºé”™: {str(e)}")
            return False

    def _is_supported_format(self, format_lower: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥æ ¼å¼"""
        supported_formats = ["csv", "parquet", "pickle", "json", "excel", "xlsx", "hdf5"]
        return format_lower in supported_formats

    def _execute_save_operation(
        self, df: pd.DataFrame, file_path: Path, format_lower: str, **kwargs
    ):
        """æ‰§è¡Œå…·ä½“çš„ä¿å­˜æ“ä½œ"""
        if format_lower == "csv":
            df.to_csv(file_path, **kwargs)
        elif format_lower == "parquet":
            df.to_parquet(file_path, **kwargs)
        elif format_lower == "pickle":
            df.to_pickle(file_path, **kwargs)
        elif format_lower == "json":
            df.to_json(file_path, **kwargs)
        elif format_lower in ["excel", "xlsx"]:
            df.to_excel(file_path, **kwargs)
        elif format_lower == "hdf5":
            store_key = kwargs.pop("key", "data")
            df.to_hdf(file_path, key=store_key, **kwargs)

    def _create_backup(self, file_path: Path) -> None:
        """
        åˆ›å»ºæ–‡ä»¶å¤‡ä»½

        å‚æ•°:
            file_path: åŸæ–‡ä»¶è·¯å¾„
        """
        try:
            backup_dir = file_path.parent / "backups"
            backup_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
            backup_path = backup_dir / backup_name

            # å¤åˆ¶æ–‡ä»¶
            import shutil

            shutil.copy2(file_path, backup_path)
            print(f"ğŸ“ å¤‡ä»½å·²åˆ›å»º: {backup_path}")

        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå¤‡ä»½å¤±è´¥: {str(e)}")

    def _save_metadata(self, df: pd.DataFrame, file_path: Path) -> None:
        """
        ä¿å­˜æ•°æ®å…ƒä¿¡æ¯

        å‚æ•°:
            df: DataFrame
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        try:
            metadata = {
                "filename": file_path.name,
                "created_at": datetime.now().isoformat(),
                "shape": df.shape,
                "columns": list(df.columns),
                "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
                "memory_usage_mb": round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2),
                "null_counts": df.isnull().sum().to_dict(),
                "index_type": str(type(df.index).__name__),
            }

            # æ•°å€¼åˆ—ç»Ÿè®¡
            numeric_cols = df.select_dtypes(include=["number"]).columns
            if len(numeric_cols) > 0:
                metadata["numeric_summary"] = df[numeric_cols].describe().to_dict()

            # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
            metadata_path = file_path.with_suffix(".metadata.json")
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {str(e)}")

    def save_multiple_formats(
        self,
        df: pd.DataFrame,
        base_filename: str,
        formats: List[str],
        subdirectory: Optional[str] = None,
    ) -> Dict[str, bool]:
        """
        å°†æ•°æ®ä¿å­˜ä¸ºå¤šç§æ ¼å¼

        å‚æ•°:
            df: è¦ä¿å­˜çš„DataFrame
            base_filename: åŸºç¡€æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰
            formats: æ ¼å¼åˆ—è¡¨
            subdirectory: å­ç›®å½•

        è¿”å›:
            Dict[str, bool]: å„æ ¼å¼çš„ä¿å­˜ç»“æœ
        """
        results = {}

        for fmt in formats:
            # æ„å»ºæ–‡ä»¶å
            if fmt.lower() == "excel":
                filename = f"{base_filename}.xlsx"
            else:
                filename = f"{base_filename}.{fmt.lower()}"

            # ä¿å­˜
            success = self.save_data(df, filename, fmt, subdirectory)
            results[fmt] = success

        return results

    def batch_save(
        self,
        dataframes: Dict[str, pd.DataFrame],
        file_format: str = "csv",
        subdirectory: Optional[str] = None,
    ) -> Dict[str, bool]:
        """
        æ‰¹é‡ä¿å­˜å¤šä¸ªDataFrame

        å‚æ•°:
            dataframes: {æ–‡ä»¶å: DataFrame} å­—å…¸
            file_format: æ–‡ä»¶æ ¼å¼
            subdirectory: å­ç›®å½•

        è¿”å›:
            Dict[str, bool]: å„æ–‡ä»¶çš„ä¿å­˜ç»“æœ
        """
        results = {}

        for filename, df in dataframes.items():
            # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®æ‰©å±•å
            if not filename.endswith(f".{file_format}"):
                filename = f"{filename}.{file_format}"

            success = self.save_data(df, filename, file_format, subdirectory)
            results[filename] = success

        return results

    def get_saved_files_info(self, subdirectory: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–å·²ä¿å­˜æ–‡ä»¶çš„ä¿¡æ¯

        å‚æ•°:
            subdirectory: å­ç›®å½•

        è¿”å›:
            List[Dict]: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        search_dir = self.base_output_dir
        if subdirectory:
            search_dir = search_dir / subdirectory

        if not search_dir.exists():
            return []

        files_info = []

        for file_path in search_dir.iterdir():
            if file_path.is_file() and not file_path.name.startswith("."):
                try:
                    stat = file_path.stat()
                    info = {
                        "filename": file_path.name,
                        "path": str(file_path),
                        "size_mb": round(stat.st_size / (1024 * 1024), 2),
                        "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "extension": file_path.suffix,
                    }

                    # å°è¯•åŠ è½½å…ƒæ•°æ®
                    metadata_path = file_path.with_suffix(f"{file_path.suffix}.metadata.json")
                    if metadata_path.exists():
                        with open(metadata_path, "r", encoding="utf-8") as f:
                            metadata = json.load(f)
                            info["metadata"] = metadata

                    files_info.append(info)

                except Exception as e:
                    print(f"âš ï¸ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")

        return sorted(files_info, key=lambda x: x["modified"], reverse=True)

    def cleanup_old_files(
        self, subdirectory: Optional[str] = None, days_old: int = 30, file_pattern: str = "*"
    ) -> int:
        """
        æ¸…ç†æ—§æ–‡ä»¶

        å‚æ•°:
            subdirectory: å­ç›®å½•
            days_old: ä¿ç•™å¤©æ•°
            file_pattern: æ–‡ä»¶æ¨¡å¼

        è¿”å›:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        search_dir = self._get_search_directory(subdirectory)
        if not search_dir.exists():
            return 0

        cutoff_time = self._calculate_cutoff_time(days_old)

        try:
            cleaned_count = self._remove_old_files(search_dir, file_pattern, cutoff_time)
            print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
            return cleaned_count

        except Exception as e:
            print(f"âŒ æ¸…ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return 0

    def _get_search_directory(self, subdirectory: Optional[str]) -> Path:
        """è·å–æœç´¢ç›®å½•"""
        search_dir = self.base_output_dir
        if subdirectory:
            search_dir = search_dir / subdirectory
        return search_dir

    def _calculate_cutoff_time(self, days_old: int) -> float:
        """è®¡ç®—æˆªæ­¢æ—¶é—´"""
        import time

        return time.time() - (days_old * 24 * 60 * 60)

    def _remove_old_files(self, search_dir: Path, file_pattern: str, cutoff_time: float) -> int:
        """ç§»é™¤æ—§æ–‡ä»¶"""
        cleaned_count = 0

        for file_path in search_dir.glob(file_pattern):
            if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                self._delete_file_and_metadata(file_path)
                cleaned_count += 1
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {file_path.name}")

        return cleaned_count

    def _delete_file_and_metadata(self, file_path: Path):
        """åˆ é™¤æ–‡ä»¶åŠå…¶å…ƒæ•°æ®"""
        file_path.unlink()

        # åˆ é™¤å¯¹åº”çš„å…ƒæ•°æ®æ–‡ä»¶
        metadata_path = file_path.with_suffix(f"{file_path.suffix}.metadata.json")
        if metadata_path.exists():
            metadata_path.unlink()


class ProcessedDataExporter:
    """å¤„ç†åæ•°æ®å¯¼å‡ºå™¨"""

    def __init__(self, data_saver: Optional[DataSaver] = None):
        """
        åˆå§‹åŒ–å¯¼å‡ºå™¨

        å‚æ•°:
            data_saver: æ•°æ®ä¿å­˜å™¨å®ä¾‹
        """
        self.data_saver = data_saver or DataSaver()

    def export_ohlcv_data(
        self, df: pd.DataFrame, symbol: str, timeframe: str = "1h", include_indicators: bool = True
    ) -> bool:
        """
        å¯¼å‡ºOHLCVæ•°æ®

        å‚æ•°:
            df: OHLCVæ•°æ®
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            include_indicators: æ˜¯å¦åŒ…å«æŠ€æœ¯æŒ‡æ ‡

        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸ
        """
        filename = f"{symbol}_{timeframe}_ohlcv"
        if include_indicators:
            filename += "_with_indicators"

        return self.data_saver.save_data(
            df, f"{filename}.csv", subdirectory="ohlcv_data", add_timestamp=True
        )

    def export_signals_data(self, df: pd.DataFrame, strategy_name: str) -> bool:
        """
        å¯¼å‡ºäº¤æ˜“ä¿¡å·æ•°æ®

        å‚æ•°:
            df: ä¿¡å·æ•°æ®
            strategy_name: ç­–ç•¥åç§°

        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸ
        """
        filename = f"{strategy_name}_signals"

        return self.data_saver.save_data(
            df, f"{filename}.csv", subdirectory="signals", add_timestamp=True
        )

    def export_backtest_results(self, results: Dict[str, Any], strategy_name: str) -> bool:
        """
        å¯¼å‡ºå›æµ‹ç»“æœ

        å‚æ•°:
            results: å›æµ‹ç»“æœå­—å…¸
            strategy_name: ç­–ç•¥åç§°

        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            filename = f"{strategy_name}_backtest_results.json"

            output_dir = self.data_saver.base_output_dir / "backtest_results"
            output_dir.mkdir(exist_ok=True)

            file_path = output_dir / filename

            # ç¡®ä¿æ•°æ®å¯åºåˆ—åŒ–
            serializable_results = self._make_serializable(results)

            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)

            print(f"âœ… å›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {file_path}")
            return True

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå›æµ‹ç»“æœå¤±è´¥: {str(e)}")
            return False

    def _make_serializable(self, data: Any) -> Any:
        """
        ä½¿æ•°æ®å¯åºåˆ—åŒ–

        å‚æ•°:
            data: è¾“å…¥æ•°æ®

        è¿”å›:
            å¯åºåˆ—åŒ–çš„æ•°æ®
        """
        if isinstance(data, dict):
            return {key: self._make_serializable(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._make_serializable(item) for item in data]
        elif isinstance(data, (pd.Series, pd.DataFrame)):
            return data.to_dict()
        elif isinstance(data, (pd.Timestamp, datetime)):
            return data.isoformat()
        elif isinstance(data, (int, float, str, bool)) or data is None:
            return data
        else:
            return str(data)


# ä¾¿æ·å‡½æ•°
def save_processed_data(
    df: pd.DataFrame, output_path: str, file_format: str = "csv", **kwargs
) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¿å­˜å¤„ç†åçš„æ•°æ®

    å‚æ•°:
        df: è¦ä¿å­˜çš„DataFrame
        output_path: è¾“å‡ºè·¯å¾„
        file_format: æ–‡ä»¶æ ¼å¼
        **kwargs: å…¶ä»–å‚æ•°

    è¿”å›:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # æ ¹æ®æ–‡ä»¶æ ¼å¼ä¿å­˜æ•°æ®
        if file_format.lower() == "csv":
            df.to_csv(output_path, **kwargs)
        elif file_format.lower() == "pickle":
            df.to_pickle(output_path, **kwargs)
        elif file_format.lower() == "parquet":
            df.to_parquet(output_path, **kwargs)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
            return False

        print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ° {output_path}")
        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return False
