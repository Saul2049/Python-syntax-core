#!/usr/bin/env python3
"""
æµ‹è¯•ç”¨ä¾‹ç˜¦èº«æ‰§è¡Œè„šæœ¬
æ ¹æ®åˆ†æç»“æœæ‰§è¡Œå®é™…çš„æ¸…ç†ã€å½’æ¡£ã€å‚æ•°åŒ–æ“ä½œ
"""

import os
import re
import shutil
from pathlib import Path


class TestCleanupExecutor:
    def __init__(self):
        self.base_dir = Path(".")
        self.tests_dir = Path("tests")
        self.archive_dir = Path("tests/archive")
        self.deprecated_dir = self.archive_dir / "deprecated"
        self.old_versions_dir = self.archive_dir / "old_versions"

        # éœ€è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆæ˜ç¡®æ ‡è®°ä¸ºbrokenã€tempç­‰ï¼‰
        self.files_to_delete = ["test_trading_loop_coverage_BROKEN.py"]

        # éœ€è¦å½’æ¡£çš„æ¨¡å¼
        self.patterns_to_archive = [
            "test_.*_demo_.*",
            "test_.*_temp_.*",
            "test_.*_old_.*",
            "test_.*legacy.*",
        ]

        # éœ€è¦å‚æ•°åŒ–çš„é‡å¤æµ‹è¯•ç»„
        self.parametrize_groups = [
            {
                "pattern": "test_init_with_.*_dir",
                "files": [
                    "test_data_saver_enhanced_fixed.py",
                    "test_data_saver_enhanced.py",
                    "test_data_saver_final.py",
                ],
                "base_name": "test_init_with_dir",
            },
            {
                "pattern": "test_save_data_.*_format",
                "files": [
                    "test_data_saver_enhanced_fixed.py",
                    "test_data_saver_enhanced.py",
                    "test_data_saver_final.py",
                ],
                "base_name": "test_save_data_format",
            },
        ]

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        print("ğŸ“ åˆ›å»ºå½’æ¡£ç›®å½•ç»“æ„...")

        self.archive_dir.mkdir(exist_ok=True)
        self.deprecated_dir.mkdir(exist_ok=True)
        self.old_versions_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        (self.tests_dir / "logs").mkdir(exist_ok=True)

        print("âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")

    def backup_current_tests(self):
        """å¤‡ä»½å½“å‰æµ‹è¯•ç›®å½•"""
        print("ğŸ’¾ åˆ›å»ºæµ‹è¯•æ–‡ä»¶å¤‡ä»½...")

        backup_dir = Path("tests_backup_before_cleanup")
        if backup_dir.exists():
            shutil.rmtree(backup_dir)

        shutil.copytree(
            self.tests_dir, backup_dir, ignore=shutil.ignore_patterns("__pycache__", "*.pyc")
        )
        print(f"âœ… å¤‡ä»½å®Œæˆ: {backup_dir}")

    def delete_broken_tests(self):
        """åˆ é™¤æ˜ç¡®æ ‡è®°ä¸ºbrokençš„æµ‹è¯•æ–‡ä»¶"""
        print("ğŸ—‘ï¸  åˆ é™¤æ˜ç¡®æ— ç”¨çš„æµ‹è¯•æ–‡ä»¶...")

        deleted_count = 0
        for file_name in self.files_to_delete:
            file_path = self.tests_dir / file_name
            if file_path.exists():
                file_path.unlink()
                print(f"   åˆ é™¤: {file_name}")
                deleted_count += 1

        print(f"âœ… åˆ é™¤äº† {deleted_count} ä¸ªæ— ç”¨æµ‹è¯•æ–‡ä»¶")

    def archive_old_tests(self):
        """å½’æ¡£é™ˆæ—§çš„æµ‹è¯•æ–‡ä»¶"""
        print("ğŸ“¦ å½’æ¡£é™ˆæ—§æµ‹è¯•æ–‡ä»¶...")

        archived_count = 0
        for test_file in self.tests_dir.glob("test_*.py"):
            file_name = test_file.name.lower()

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…éœ€è¦å½’æ¡£çš„æ¨¡å¼
            should_archive = any(
                re.search(pattern, file_name) for pattern in self.patterns_to_archive
            )

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é™ˆæ—§å…³é”®è¯
            old_keywords = ["demo", "temp", "old", "legacy", "deprecated"]
            has_old_keywords = any(keyword in file_name for keyword in old_keywords)

            if should_archive or has_old_keywords:
                dest_path = self.deprecated_dir / test_file.name
                shutil.move(str(test_file), str(dest_path))
                print(f"   å½’æ¡£: {test_file.name} -> {dest_path}")
                archived_count += 1

        print(f"âœ… å½’æ¡£äº† {archived_count} ä¸ªé™ˆæ—§æµ‹è¯•æ–‡ä»¶")

    def identify_duplicate_files(self):
        """è¯†åˆ«é‡å¤çš„æµ‹è¯•æ–‡ä»¶ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰"""
        print("ğŸ” è¯†åˆ«é‡å¤æµ‹è¯•æ–‡ä»¶...")

        duplicate_groups = []

        # æŸ¥æ‰¾data_saverç›¸å…³çš„é‡å¤æ–‡ä»¶
        data_saver_files = [
            "test_data_saver_enhanced.py",
            "test_data_saver_enhanced_fixed.py",
            "test_data_saver_enhanced_part2.py",
            "test_data_saver_final.py",
        ]

        existing_data_saver_files = [f for f in data_saver_files if (self.tests_dir / f).exists()]
        if len(existing_data_saver_files) > 1:
            duplicate_groups.append(
                {
                    "type": "data_saver",
                    "files": existing_data_saver_files,
                    "keep": "test_data_saver_final.py",  # ä¿ç•™æœ€ç»ˆç‰ˆæœ¬
                }
            )

        # æŸ¥æ‰¾metrics_collectorç›¸å…³çš„é‡å¤æ–‡ä»¶
        metrics_files = [
            "test_metrics_collector_enhanced.py",
            "test_metrics_collector_enhanced_fixed.py",
            "test_metrics_collector_enhanced_part2.py",
        ]

        existing_metrics_files = [f for f in metrics_files if (self.tests_dir / f).exists()]
        if len(existing_metrics_files) > 1:
            duplicate_groups.append(
                {
                    "type": "metrics_collector",
                    "files": existing_metrics_files,
                    "keep": "test_metrics_collector_enhanced_fixed.py",  # ä¿ç•™ä¿®å¤ç‰ˆæœ¬
                }
            )

        return duplicate_groups

    def merge_duplicate_files(self):
        """åˆå¹¶é‡å¤çš„æµ‹è¯•æ–‡ä»¶"""
        print("ğŸ”„ åˆå¹¶é‡å¤æµ‹è¯•æ–‡ä»¶...")

        duplicate_groups = self.identify_duplicate_files()
        merged_count = 0

        for group in duplicate_groups:
            print(f"   å¤„ç† {group['type']} é‡å¤æ–‡ä»¶ç»„...")

            keep_file = group["keep"]
            files_to_merge = [f for f in group["files"] if f != keep_file]

            # å°†å…¶ä»–æ–‡ä»¶ç§»åŠ¨åˆ°old_versionsç›®å½•
            for file_name in files_to_merge:
                source_path = self.tests_dir / file_name
                if source_path.exists():
                    dest_path = self.old_versions_dir / file_name
                    shutil.move(str(source_path), str(dest_path))
                    print(f"     ç§»åŠ¨: {file_name} -> old_versions/")
                    merged_count += 1

        print(f"âœ… åˆå¹¶äº† {merged_count} ä¸ªé‡å¤æµ‹è¯•æ–‡ä»¶")

    def create_parametrized_test_example(self):
        """åˆ›å»ºå‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹"""
        print("âš¡ åˆ›å»ºå‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹...")

        example_content = '''"""
å‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹ - æ›¿ä»£é‡å¤æµ‹è¯•çš„ç°ä»£åŒ–æ–¹å¼
"""

import pytest
import pandas as pd
from pathlib import Path


class TestDataSaverParametrized:
    """å‚æ•°åŒ–çš„DataSaveræµ‹è¯• - æ›¿ä»£å¤šä¸ªé‡å¤æµ‹è¯•æ–‡ä»¶"""

    @pytest.mark.parametrize("dir_type,dir_value,expected", [
        ("default", None, "data"),
        ("string", "custom_data", "custom_data"),
        ("path", Path("path_data"), "path_data"),
    ])
    def test_init_with_dir(self, dir_type, dir_value, expected):
        """å‚æ•°åŒ–æµ‹è¯•ç›®å½•åˆå§‹åŒ– - æ›¿ä»£3ä¸ªé‡å¤æµ‹è¯•"""
        from src.data.data_saver import DataSaver

        if dir_type == "default":
            saver = DataSaver()
        else:
            saver = DataSaver(data_dir=dir_value)

        assert expected in str(saver.data_dir)

    @pytest.mark.parametrize("format_type,extension", [
        ("csv", ".csv"),
        ("parquet", ".parquet"),
        ("pickle", ".pkl"),
        ("json", ".json"),
        ("excel", ".xlsx"),
    ])
    def test_save_data_formats(self, format_type, extension, tmp_path):
        """å‚æ•°åŒ–æµ‹è¯•ä¿å­˜æ ¼å¼ - æ›¿ä»£5ä¸ªé‡å¤æµ‹è¯•"""
        from src.data.data_saver import DataSaver

        saver = DataSaver(data_dir=tmp_path)
        data = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})

        if format_type == "excel":
            pytest.importorskip("openpyxl")

        file_path = saver.save_data(data, f"test_file", format_type)
        assert file_path.suffix == extension
        assert file_path.exists()

    @pytest.mark.parametrize("threshold,expected_files", [
        (0, 0),  # åˆ é™¤æ‰€æœ‰æ–‡ä»¶
        (1, 1),  # ä¿ç•™1ä¸ªæœ€æ–°æ–‡ä»¶
        (3, 3),  # ä¿ç•™3ä¸ªæœ€æ–°æ–‡ä»¶
    ])
    def test_cleanup_old_files_thresholds(self, threshold, expected_files, tmp_path):
        """å‚æ•°åŒ–æµ‹è¯•æ¸…ç†é˜ˆå€¼ - æ›¿ä»£å¤šä¸ªé‡å¤æµ‹è¯•"""
        from src.data.data_saver import DataSaver
        import time

        saver = DataSaver(data_dir=tmp_path)

        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
        for i in range(5):
            file_path = tmp_path / f"test_file_{i}.csv"
            file_path.write_text("test,data\\n1,2")
            time.sleep(0.1)  # ç¡®ä¿æ–‡ä»¶æ—¶é—´ä¸åŒ

        saver.cleanup_old_files(days_old=0, max_files=threshold)
        remaining_files = list(tmp_path.glob("*.csv"))
        assert len(remaining_files) == expected_files


class TestTradingEngineParametrized:
    """å‚æ•°åŒ–çš„äº¤æ˜“å¼•æ“æµ‹è¯•"""

    @pytest.mark.parametrize("engine_status,expected_result", [
        ("running", True),
        ("stopped", False),
        ("error", False),
        ("initializing", False),
    ])
    def test_engine_status_checks(self, engine_status, expected_result):
        """å‚æ•°åŒ–æµ‹è¯•å¼•æ“çŠ¶æ€æ£€æŸ¥"""
        # æ¨¡æ‹Ÿæµ‹è¯•å®ç°
        result = engine_status == "running"
        assert result == expected_result

    @pytest.mark.parametrize("market_condition,signal_strength,expected_action", [
        ("bullish", 0.8, "buy"),
        ("bearish", 0.8, "sell"),
        ("sideways", 0.5, "hold"),
        ("volatile", 0.3, "hold"),
    ])
    def test_trading_decisions(self, market_condition, signal_strength, expected_action):
        """å‚æ•°åŒ–æµ‹è¯•äº¤æ˜“å†³ç­–é€»è¾‘"""
        # æ¨¡æ‹Ÿäº¤æ˜“å†³ç­–é€»è¾‘
        if signal_strength < 0.6:
            action = "hold"
        elif market_condition == "bullish":
            action = "buy"
        elif market_condition == "bearish":
            action = "sell"
        else:
            action = "hold"

        assert action == expected_action
'''

        example_file = self.tests_dir / "test_parametrized_examples.py"
        with open(example_file, "w", encoding="utf-8") as f:
            f.write(example_content)

        print(f"âœ… åˆ›å»ºå‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹: {example_file}")

    def create_conftest_modernization(self):
        """ç°ä»£åŒ–conftest.pyé…ç½®"""
        print("âš™ï¸  ç°ä»£åŒ–conftest.pyé…ç½®...")

        modern_conftest = '''"""
ç°ä»£åŒ–çš„pytesté…ç½®æ–‡ä»¶
æä¾›å…±ç”¨fixtureså’Œæµ‹è¯•é…ç½®
"""

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
import shutil
from unittest.mock import Mock, patch


# ============================================================================
# æ•°æ® Fixtures
# ============================================================================

@pytest.fixture
def sample_ohlcv_data():
    """æä¾›æ ‡å‡†çš„OHLCVæµ‹è¯•æ•°æ®"""
    dates = pd.date_range('2023-01-01', periods=100, freq='1H')
    np.random.seed(42)  # ç¡®ä¿å¯é‡ç°

    base_price = 100
    returns = np.random.normal(0, 0.02, 100)
    prices = base_price * np.cumprod(1 + returns)

    return pd.DataFrame({
        'timestamp': dates,
        'open': prices * np.random.uniform(0.99, 1.01, 100),
        'high': prices * np.random.uniform(1.00, 1.02, 100),
        'low': prices * np.random.uniform(0.98, 1.00, 100),
        'close': prices,
        'volume': np.random.randint(1000, 10000, 100)
    }).set_index('timestamp')


@pytest.fixture
def empty_dataframe():
    """ç©ºçš„DataFrame"""
    return pd.DataFrame()


@pytest.fixture
def invalid_data():
    """åŒ…å«æ— æ•ˆæ•°æ®çš„DataFrame"""
    return pd.DataFrame({
        'price': [1, 2, np.nan, np.inf, -np.inf, 5],
        'volume': [100, np.nan, 300, 400, 500, 600]
    })


# ============================================================================
# æ–‡ä»¶ç³»ç»Ÿ Fixtures
# ============================================================================

@pytest.fixture
def temp_data_dir():
    """ä¸´æ—¶æ•°æ®ç›®å½•ï¼Œæµ‹è¯•åè‡ªåŠ¨æ¸…ç†"""
    temp_dir = Path(tempfile.mkdtemp())
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def mock_config():
    """æ¨¡æ‹Ÿé…ç½®å¯¹è±¡"""
    config = Mock()
    config.data_dir = "test_data"
    config.api_key = "test_api_key"
    config.secret_key = "test_secret_key"
    config.enable_monitoring = True
    config.log_level = "INFO"
    return config


# ============================================================================
# ç½‘ç»œå’ŒAPI Fixtures
# ============================================================================

@pytest.fixture
def mock_binance_client():
    """æ¨¡æ‹ŸBinanceå®¢æˆ·ç«¯"""
    client = Mock()
    client.get_ticker.return_value = {'symbol': 'BTCUSDT', 'price': '50000.00'}
    client.get_account.return_value = {'balances': []}
    client.create_order.return_value = {'orderId': 12345, 'status': 'FILLED'}
    return client


@pytest.fixture
def mock_network_response():
    """æ¨¡æ‹Ÿç½‘ç»œå“åº”"""
    response = Mock()
    response.status_code = 200
    response.json.return_value = {'status': 'success', 'data': {}}
    return response


# ============================================================================
# äº¤æ˜“ç›¸å…³ Fixtures
# ============================================================================

@pytest.fixture
def sample_trade_data():
    """ç¤ºä¾‹äº¤æ˜“æ•°æ®"""
    return {
        'symbol': 'BTCUSDT',
        'side': 'BUY',
        'quantity': 0.1,
        'price': 50000.00,
        'timestamp': pd.Timestamp.now()
    }


@pytest.fixture
def mock_trading_engine():
    """æ¨¡æ‹Ÿäº¤æ˜“å¼•æ“"""
    engine = Mock()
    engine.status = 'running'
    engine.get_balance.return_value = 10000.0
    engine.execute_trade.return_value = {'status': 'success'}
    engine.analyze_market.return_value = {'signal': 'buy', 'strength': 0.8}
    return engine


# ============================================================================
# æ€§èƒ½å’Œç›‘æ§ Fixtures
# ============================================================================

@pytest.fixture
def mock_metrics_collector():
    """æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†å™¨"""
    collector = Mock()
    collector.record_trade = Mock()
    collector.record_error = Mock()
    collector.get_metrics = Mock(return_value={})
    return collector


# ============================================================================
# æµ‹è¯•æ ‡è®°é…ç½®
# ============================================================================

def pytest_configure(config):
    """é…ç½®è‡ªå®šä¹‰æ ‡è®°"""
    config.addinivalue_line(
        "markers", "unit: å•å…ƒæµ‹è¯•æ ‡è®°"
    )
    config.addinivalue_line(
        "markers", "integration: é›†æˆæµ‹è¯•æ ‡è®°"
    )
    config.addinivalue_line(
        "markers", "slow: æ…¢é€Ÿæµ‹è¯•æ ‡è®°"
    )


# ============================================================================
# æµ‹è¯•æ•°æ®æ¸…ç†
# ============================================================================

@pytest.fixture(autouse=True)
def cleanup_test_files():
    """è‡ªåŠ¨æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    yield
    # æµ‹è¯•åæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    temp_patterns = ['test_*.tmp', 'temp_*.csv', 'debug_*.log']
    for pattern in temp_patterns:
        for file_path in Path('.').glob(pattern):
            try:
                file_path.unlink()
            except OSError:
                pass


# ============================================================================
# æ€§èƒ½æµ‹è¯•å·¥å…·
# ============================================================================

@pytest.fixture
def performance_monitor():
    """æ€§èƒ½ç›‘æ§å·¥å…·"""
    import time
    import psutil

    class PerformanceMonitor:
        def __init__(self):
            self.start_time = None
            self.start_memory = None

        def start(self):
            self.start_time = time.time()
            self.start_memory = psutil.Process().memory_info().rss

        def stop(self):
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss

            return {
                'duration': end_time - self.start_time,
                'memory_delta': end_memory - self.start_memory
            }

    return PerformanceMonitor()
'''

        conftest_path = self.tests_dir / "conftest_modern.py"
        with open(conftest_path, "w", encoding="utf-8") as f:
            f.write(modern_conftest)

        print(f"âœ… åˆ›å»ºç°ä»£åŒ–conftest: {conftest_path}")

    def create_test_runner_scripts(self):
        """åˆ›å»ºåˆ†å±‚æµ‹è¯•è¿è¡Œè„šæœ¬"""
        print("ğŸš€ åˆ›å»ºåˆ†å±‚æµ‹è¯•è¿è¡Œè„šæœ¬...")

        # å¿«é€Ÿæµ‹è¯•è„šæœ¬
        fast_test_script = """#!/bin/bash
# å¿«é€Ÿæµ‹è¯• - åªè¿è¡Œå•å…ƒæµ‹è¯•å’Œæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•

echo "ğŸš€ è¿è¡Œå¿«é€Ÿæµ‹è¯•å¥—ä»¶..."

pytest tests/ \\
    -m "unit and not slow" \\
    --maxfail=5 \\
    --tb=short \\
    --durations=5 \\
    -v

echo "âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ"
"""

        # å®Œæ•´æµ‹è¯•è„šæœ¬
        full_test_script = """#!/bin/bash
# å®Œæ•´æµ‹è¯• - åŒ…å«æ‰€æœ‰æµ‹è¯•

echo "ğŸ§ª è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶..."

pytest tests/ \\
    --cov=src \\
    --cov-report=html \\
    --cov-report=term-missing \\
    --durations=10 \\
    -v

echo "âœ… å®Œæ•´æµ‹è¯•å®Œæˆ"
"""

        # é›†æˆæµ‹è¯•è„šæœ¬
        integration_test_script = """#!/bin/bash
# é›†æˆæµ‹è¯• - åªè¿è¡Œé›†æˆæµ‹è¯•

echo "ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•..."

pytest tests/ \\
    -m "integration" \\
    --tb=short \\
    -v

echo "âœ… é›†æˆæµ‹è¯•å®Œæˆ"
"""

        scripts = [
            ("run_fast_tests.sh", fast_test_script),
            ("run_full_tests.sh", full_test_script),
            ("run_integration_tests.sh", integration_test_script),
        ]

        for script_name, script_content in scripts:
            script_path = Path(script_name)
            with open(script_path, "w") as f:
                f.write(script_content)
            os.chmod(script_path, 0o755)
            print(f"   åˆ›å»º: {script_name}")

        print("âœ… æµ‹è¯•è¿è¡Œè„šæœ¬åˆ›å»ºå®Œæˆ")

    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæ¸…ç†æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæœ€ç»ˆæ¸…ç†æŠ¥å‘Š...")

        # ç»Ÿè®¡å½“å‰æµ‹è¯•æ–‡ä»¶æ•°é‡
        current_test_files = list(self.tests_dir.glob("test_*.py"))
        archived_files = (
            list(self.archive_dir.rglob("test_*.py")) if self.archive_dir.exists() else []
        )

        report = f"""# æµ‹è¯•ç”¨ä¾‹ç˜¦èº«å®ŒæˆæŠ¥å‘Š

## ğŸ“Š æ¸…ç†ç»Ÿè®¡

### å½“å‰çŠ¶æ€
- æ´»è·ƒæµ‹è¯•æ–‡ä»¶: {len(current_test_files)} ä¸ª
- å½’æ¡£æµ‹è¯•æ–‡ä»¶: {len(archived_files)} ä¸ª
- å¤‡ä»½ä½ç½®: tests_backup_before_cleanup/

## ğŸ¯ å®Œæˆçš„æ“ä½œ

### âœ… å·²å®Œæˆ
- [x] åˆ é™¤æ˜ç¡®æ— ç”¨çš„æµ‹è¯• (broken, tempç­‰)
- [x] å½’æ¡£é™ˆæ—§æµ‹è¯•åˆ° archive/deprecated/
- [x] åˆå¹¶é‡å¤æµ‹è¯•æ–‡ä»¶åˆ° archive/old_versions/
- [x] åˆ›å»ºå‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹
- [x] ç°ä»£åŒ– pytest é…ç½®
- [x] åˆ›å»ºåˆ†å±‚æµ‹è¯•è¿è¡Œè„šæœ¬

### ğŸ“ æ¨èåç»­æ“ä½œ

1. **å‚æ•°åŒ–ç°æœ‰é‡å¤æµ‹è¯•**
2. **æ·»åŠ æµ‹è¯•æ ‡è®°**
3. **ä½¿ç”¨æ–°é…ç½®**

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

1. éªŒè¯æ¸…ç†åçš„æµ‹è¯•å¥—ä»¶è¿è¡Œæ­£å¸¸
2. æ›´æ–°CIé…ç½®ä½¿ç”¨æ–°çš„åˆ†å±‚æµ‹è¯•
3. é€æ­¥ä¸ºç°æœ‰æµ‹è¯•æ·»åŠ é€‚å½“çš„æ ‡è®°

æµ‹è¯•ç˜¦èº«å®Œæˆæ—¶é—´: {self.get_current_time()}
"""

        with open("TEST_CLEANUP_COMPLETION_REPORT.md", "w", encoding="utf-8") as f:
            f.write(report)

        print("âœ… æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: TEST_CLEANUP_COMPLETION_REPORT.md")

    def get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        import datetime

        return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def run_cleanup(self):
        """æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æ¸…ç†æµç¨‹"""
        print("ğŸ§¹ å¼€å§‹æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ç˜¦èº«...")
        print("=" * 60)

        try:
            self.setup_directories()
            self.backup_current_tests()
            self.delete_broken_tests()
            self.archive_old_tests()
            self.merge_duplicate_files()
            self.create_parametrized_test_example()
            self.create_conftest_modernization()
            self.create_test_runner_scripts()
            self.generate_final_report()

            print("\n" + "=" * 60)
            print("ğŸ‰ æµ‹è¯•ç”¨ä¾‹ç˜¦èº«å®Œæˆï¼")
            print("\næ¨èä¸‹ä¸€æ­¥æ“ä½œ:")
            print("1. éªŒè¯æ¸…ç†ç»“æœ: pytest tests/ --collect-only")
            print("2. è¿è¡Œå¿«é€Ÿæµ‹è¯•: ./run_fast_tests.sh")
            print("3. æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š: cat TEST_CLEANUP_COMPLETION_REPORT.md")

        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            print("ğŸ’¾ æ‰€æœ‰åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åœ¨ tests_backup_before_cleanup/")
            raise


if __name__ == "__main__":
    executor = TestCleanupExecutor()
    executor.run_cleanup()
