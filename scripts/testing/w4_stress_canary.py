#!/usr/bin/env python3
"""
W4 24å°æ—¶å‹åŠ›Canaryæµ‹è¯•
W4 24-Hour Stress Test Canary

éªŒæ”¶æ ‡å‡†:
- å…¨ç¨‹æ— WARN (RSS<40MB, GC pause P95<0.05s)
- P95å»¶è¿Ÿä¸åå¼¹ (â‰¤5.5ms)
- è‡ªåŠ¨Slack/Telegramæ±‡æŠ¥
"""

import asyncio
import json
import logging
import os
import random
import signal
import sys
import time
from typing import Any, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

import numpy as np
import psutil

from scripts.testing.w3_leak_sentinel import LeakSentinel
from src.core.gc_optimizer import GCOptimizer
from src.monitoring.metrics_collector import get_metrics_collector
from src.strategies.cache_optimized_strategy import CacheOptimizedStrategy


class W4StressCanary:
    """W4 24å°æ—¶å‹åŠ›Canaryæµ‹è¯•å™¨"""

    def __init__(self, pairs: List[str] = None, frequency_hz: int = 5):
        self.pairs = pairs or ["BTCUSDT", "ETHUSDT", "XRPUSDT"]
        self.frequency_hz = frequency_hz
        self.test_duration_hours = 24

        self.metrics = get_metrics_collector()
        self.gc_optimizer = GCOptimizer()
        self.leak_sentinel = LeakSentinel(alert_threshold_hours=1)  # æ¯å°æ—¶æ£€æŸ¥

        # W4éªŒæ”¶é˜ˆå€¼
        self.w4_thresholds = {
            "max_rss_mb": 40,
            "max_gc_pause_p95_ms": 50,
            "max_latency_p95_ms": 5.5,
            "max_fd_count": 500,
        }

        # æµ‹è¯•çŠ¶æ€
        self.test_running = False
        self.start_time = None
        self.alerts = []
        self.performance_samples = []
        self.hourly_reports = []

        # ç­–ç•¥å’Œæ•°æ®
        self.strategies = {}
        self.price_feeds = {}

        self.logger = logging.getLogger(__name__)

    async def initialize_test_environment(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.logger.info("ğŸš€ åˆå§‹åŒ–W4å‹åŠ›æµ‹è¯•ç¯å¢ƒ...")

        # 1. åˆå§‹åŒ–ç­–ç•¥
        for pair in self.pairs:
            config = {"symbols": [pair], "risk_limit": 1000}
            self.strategies[pair] = CacheOptimizedStrategy(config)

        # 2. å¯åŠ¨GCä¼˜åŒ–å™¨
        self.gc_optimizer.install_gc_callbacks()

        # 3. åˆå§‹åŒ–ä»·æ ¼æ•°æ®ç”Ÿæˆå™¨
        self._initialize_price_feeds()

        # 4. å¯åŠ¨Prometheusç›‘æ§
        self.metrics.start_server()

        self.logger.info(
            f"âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - äº¤æ˜“å¯¹: {self.pairs}, é¢‘ç‡: {self.frequency_hz}Hz"
        )

    def _initialize_price_feeds(self):
        """åˆå§‹åŒ–ä»·æ ¼æ•°æ®æµ"""
        base_prices = {"BTCUSDT": 45000, "ETHUSDT": 3000, "XRPUSDT": 0.6}

        for pair in self.pairs:
            self.price_feeds[pair] = {
                "current_price": base_prices.get(pair, 100),
                "volatility": 0.02,  # 2%æ³¢åŠ¨ç‡
                "trend": 0.0001,  # è½»å¾®ä¸Šæ¶¨è¶‹åŠ¿
                "last_update": time.time(),
            }

    def _generate_next_price(self, pair: str) -> Dict[str, float]:
        """ç”Ÿæˆä¸‹ä¸€ä¸ªä»·æ ¼æ•°æ®"""
        feed = self.price_feeds[pair]

        # éšæœºæ¸¸èµ° + è¶‹åŠ¿
        change_pct = np.random.normal(feed["trend"], feed["volatility"])
        new_price = feed["current_price"] * (1 + change_pct)

        # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
        new_price = max(0.01, new_price)

        # ç”ŸæˆOHLCVæ•°æ®
        high = new_price * (1 + abs(np.random.normal(0, 0.005)))
        low = new_price * (1 - abs(np.random.normal(0, 0.005)))
        volume = random.randint(100, 10000)

        ohlcv = {
            "symbol": pair,
            "open": feed["current_price"],
            "high": high,
            "low": low,
            "close": new_price,
            "volume": volume,
            "timestamp": time.time(),
        }

        # æ›´æ–°å½“å‰ä»·æ ¼
        feed["current_price"] = new_price
        feed["last_update"] = time.time()

        return ohlcv

    async def run_high_frequency_load(self):
        """è¿è¡Œé«˜é¢‘äº¤æ˜“è´Ÿè½½"""
        interval = 1.0 / self.frequency_hz  # 5Hz = 0.2ç§’é—´éš”
        operation_count = 0

        self.logger.info(f"ğŸ”¥ å¼€å§‹é«˜é¢‘è´Ÿè½½æµ‹è¯• - {self.frequency_hz}Hzé¢‘ç‡")

        while self.test_running:
            start_cycle = time.time()

            # å¹¶è¡Œå¤„ç†æ‰€æœ‰äº¤æ˜“å¯¹
            tasks = []
            for pair in self.pairs:
                task = asyncio.create_task(self._process_trading_pair(pair))
                tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰å¤„ç†å®Œæˆå¹¶æµ‹é‡å»¶è¿Ÿ
            results = await asyncio.gather(*tasks, return_exceptions=True)

            cycle_duration = time.time() - start_cycle
            operation_count += len(self.pairs)

            # è®°å½•æ€§èƒ½æ ·æœ¬
            self.performance_samples.append(
                {
                    "timestamp": time.time(),
                    "cycle_duration_ms": cycle_duration * 1000,
                    "operations_in_cycle": len(self.pairs),
                    "success_count": len([r for r in results if not isinstance(r, Exception)]),
                    "error_count": len([r for r in results if isinstance(r, Exception)]),
                }
            )

            # é™åˆ¶æ€§èƒ½æ ·æœ¬æ•°é‡
            if len(self.performance_samples) > 10000:
                self.performance_samples = self.performance_samples[-8000:]

            # æ›´æ–°å»¶è¿ŸæŒ‡æ ‡
            self.metrics.observe_task_latency("trading_cycle", cycle_duration)

            # æ§åˆ¶é¢‘ç‡
            elapsed = time.time() - start_cycle
            sleep_time = max(0, interval - elapsed)

            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
            elif elapsed > interval * 2:  # å»¶è¿Ÿè¶…è¿‡2å€é—´éš”
                self.logger.warning(
                    f"âš ï¸ å¤„ç†å»¶è¿Ÿè¿‡é«˜: {elapsed*1000:.1f}ms (ç›®æ ‡: {interval*1000:.1f}ms)"
                )

    async def _process_trading_pair(self, pair: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹"""
        try:
            # ç”Ÿæˆä»·æ ¼æ•°æ®
            price_data = self._generate_next_price(pair)

            # æ›´æ–°ç­–ç•¥çª—å£æ•°æ®
            strategy = self.strategies[pair]
            ohlcv_array = np.array(
                [
                    price_data["open"],
                    price_data["high"],
                    price_data["low"],
                    price_data["close"],
                    price_data["volume"],
                ]
            )
            strategy.update_window(pair, 100, ohlcv_array)

            # ç”Ÿæˆäº¤æ˜“ä¿¡å·
            with self.metrics.measure_signal_latency():
                signal = strategy.generate_signals(pair, price_data["close"])

            # æ›´æ–°ä»·æ ¼æŒ‡æ ‡
            self.metrics.record_price_update(pair, price_data["close"], "generator")

            # æ¨¡æ‹Ÿè®¢å•å¤„ç†å»¶è¿Ÿ
            if signal["action"] != "hold":
                processing_delay = random.uniform(0.001, 0.005)  # 1-5ms
                await asyncio.sleep(processing_delay)

            return {"pair": pair, "signal": signal, "price": price_data["close"], "success": True}

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç† {pair} å¤±è´¥: {e}")
            self.metrics.record_exception("trading_processor", e)

            return {"pair": pair, "error": str(e), "success": False}

    async def monitor_w4_thresholds(self):
        """ç›‘æ§W4éªŒæ”¶é˜ˆå€¼"""
        check_interval = 300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

        while self.test_running:
            # æ”¶é›†å½“å‰æŒ‡æ ‡
            current_metrics = await self._collect_system_metrics()

            # æ£€æŸ¥W4é˜ˆå€¼
            violations = self._check_w4_violations(current_metrics)

            if violations:
                # è®°å½•å‘Šè­¦
                alert = {
                    "timestamp": time.time(),
                    "type": "w4_threshold_violation",
                    "violations": violations,
                    "metrics": current_metrics,
                }
                self.alerts.append(alert)

                # å‘é€å‘Šè­¦
                await self._send_alert(alert)

                self.logger.warning(f"ğŸš¨ W4é˜ˆå€¼è¿å: {violations}")

            await asyncio.sleep(check_interval)

    async def _collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # è¿›ç¨‹ä¿¡æ¯
            process = psutil.Process()
            memory_info = process.memory_info()

            # GCç»Ÿè®¡
            gc_report = self.gc_optimizer.get_optimization_report()

            # æ€§èƒ½ç»Ÿè®¡
            recent_samples = [
                s for s in self.performance_samples if time.time() - s["timestamp"] <= 3600
            ]  # æœ€è¿‘1å°æ—¶

            if recent_samples:
                latencies = [s["cycle_duration_ms"] for s in recent_samples]
                latencies.sort()
                p95_latency = latencies[int(len(latencies) * 0.95)] if latencies else 0
                avg_latency = sum(latencies) / len(latencies) if latencies else 0
            else:
                p95_latency = 0
                avg_latency = 0

            return {
                "timestamp": time.time(),
                "memory": {
                    "rss_mb": memory_info.rss / (1024 * 1024),
                    "vms_mb": memory_info.vms / (1024 * 1024),
                    "percent": process.memory_percent(),
                },
                "gc": {
                    "current_profile": gc_report.get("current_profile"),
                    "recent_pauses": len(self.gc_optimizer.pause_history),
                    "monitoring_active": gc_report.get("monitoring_active", False),
                },
                "performance": {
                    "p95_latency_ms": p95_latency,
                    "avg_latency_ms": avg_latency,
                    "samples_count": len(recent_samples),
                    "operations_per_second": (
                        len(recent_samples) * len(self.pairs) / 3600 if recent_samples else 0
                    ),
                },
                "file_descriptors": (
                    process.num_fds() if hasattr(process, "num_fds") else len(process.open_files())
                ),
                "connections": len(process.connections()),
                "cpu_percent": process.cpu_percent(),
            }

        except Exception as e:
            self.logger.error(f"âŒ ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            return {"timestamp": time.time(), "error": str(e)}

    def _check_w4_violations(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æŸ¥W4é˜ˆå€¼è¿å"""
        if "error" in metrics:
            return []

        violations = []

        # RSSå†…å­˜æ£€æŸ¥
        rss_mb = metrics["memory"]["rss_mb"]
        if rss_mb > self.w4_thresholds["max_rss_mb"]:
            violations.append(
                {
                    "threshold": "rss_memory",
                    "current": rss_mb,
                    "limit": self.w4_thresholds["max_rss_mb"],
                    "severity": "critical" if rss_mb > 60 else "warning",
                }
            )

        # P95å»¶è¿Ÿæ£€æŸ¥
        p95_latency = metrics["performance"]["p95_latency_ms"]
        if p95_latency > self.w4_thresholds["max_latency_p95_ms"]:
            violations.append(
                {
                    "threshold": "p95_latency",
                    "current": p95_latency,
                    "limit": self.w4_thresholds["max_latency_p95_ms"],
                    "severity": "warning",
                }
            )

        # FDæ£€æŸ¥
        fd_count = metrics["file_descriptors"]
        if fd_count > self.w4_thresholds["max_fd_count"]:
            violations.append(
                {
                    "threshold": "file_descriptors",
                    "current": fd_count,
                    "limit": self.w4_thresholds["max_fd_count"],
                    "severity": "critical" if fd_count > 800 else "warning",
                }
            )

        return violations

    async def _send_alert(self, alert: Dict[str, Any]):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„Slack/Telegram API
        # ç›®å‰åªè®°å½•æ—¥å¿—

        violations_summary = ", ".join(
            [f"{v['threshold']}={v['current']:.1f}>{v['limit']}" for v in alert["violations"]]
        )

        alert_message = f"ğŸš¨ W4å‹åŠ›æµ‹è¯•å‘Šè­¦: {violations_summary}"
        self.logger.warning(alert_message)

        # æ¨¡æ‹ŸTelegram/Slackå‘é€
        print(f"ğŸ“± å‘Šè­¦é€šçŸ¥: {alert_message}")

    async def generate_hourly_report(self):
        """ç”Ÿæˆæ¯å°æ—¶æŠ¥å‘Š"""
        report_interval = 3600  # 1å°æ—¶
        hours_completed = 0

        while self.test_running:
            await asyncio.sleep(report_interval)
            hours_completed += 1

            # ç”Ÿæˆå°æ—¶æŠ¥å‘Š
            hourly_metrics = await self._collect_system_metrics()

            # è®¡ç®—å°æ—¶å†…æ€§èƒ½ç»Ÿè®¡
            hour_start = time.time() - 3600
            hour_samples = [s for s in self.performance_samples if s["timestamp"] >= hour_start]

            hour_report = {
                "hour": hours_completed,
                "timestamp": time.time(),
                "metrics": hourly_metrics,
                "performance_summary": {
                    "total_operations": len(hour_samples) * len(self.pairs),
                    "avg_ops_per_second": len(hour_samples) * len(self.pairs) / 3600,
                    "error_rate": sum(s["error_count"] for s in hour_samples)
                    / max(1, len(hour_samples)),
                },
                "alerts_in_hour": len([a for a in self.alerts if a["timestamp"] >= hour_start]),
                "w4_status": "PASS" if not self._check_w4_violations(hourly_metrics) else "FAIL",
            }

            self.hourly_reports.append(hour_report)

            # å‘é€å°æ—¶æŠ¥å‘Š
            await self._send_hourly_notification(hour_report)

            self.logger.info(
                f"ğŸ“Š ç¬¬{hours_completed}å°æ—¶æŠ¥å‘Šå®Œæˆ - çŠ¶æ€: {hour_report['w4_status']}"
            )

    async def _send_hourly_notification(self, report: Dict[str, Any]):
        """å‘é€å°æ—¶æŠ¥å‘Šé€šçŸ¥"""
        metrics = report["metrics"]

        summary = (
            f"â° W4å‹åŠ›æµ‹è¯• - ç¬¬{report['hour']}å°æ—¶æŠ¥å‘Š\n"
            f"ğŸ“Š RSS: {metrics['memory']['rss_mb']:.1f}MB\n"
            f"âš¡ P95å»¶è¿Ÿ: {metrics['performance']['p95_latency_ms']:.1f}ms\n"
            f"ğŸ”— FD: {metrics['file_descriptors']}\n"
            f"ğŸ¯ çŠ¶æ€: {report['w4_status']}\n"
            f"âš ï¸ æœ¬å°æ—¶å‘Šè­¦: {report['alerts_in_hour']}æ¬¡"
        )

        self.logger.info(f"å°æ—¶æŠ¥å‘Š:\n{summary}")
        print(f"ğŸ“± å°æ—¶æŠ¥å‘Šé€šçŸ¥:\n{summary}")

    async def run_stress_test(self) -> bool:
        """è¿è¡Œ24å°æ—¶å‹åŠ›æµ‹è¯•"""
        self.test_running = True
        self.start_time = time.time()

        try:
            # åˆå§‹åŒ–ç¯å¢ƒ
            await self.initialize_test_environment()

            # è®¾ç½®ä¿¡å·å¤„ç†
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)

            self.logger.info("ğŸš€ å¼€å§‹W4 24å°æ—¶å‹åŠ›æµ‹è¯•")

            # å¹¶è¡Œè¿è¡Œå„ä¸ªç»„ä»¶
            tasks = [
                asyncio.create_task(self.run_high_frequency_load()),
                asyncio.create_task(self.monitor_w4_thresholds()),
                asyncio.create_task(self.generate_hourly_report()),
            ]

            # ç­‰å¾…24å°æ—¶æˆ–æ‰‹åŠ¨åœæ­¢
            end_time = self.start_time + (self.test_duration_hours * 3600)

            while self.test_running and time.time() < end_time:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                for i, task in enumerate(tasks):
                    if task.done() and task.exception():
                        self.logger.error(f"âŒ ä»»åŠ¡{i}å¼‚å¸¸é€€å‡º: {task.exception()}")
                        raise task.exception()

            # æ­£å¸¸å®Œæˆ
            self.test_running = False

            # ç­‰å¾…ä»»åŠ¡æ¸…ç†
            for task in tasks:
                if not task.done():
                    task.cancel()

            await asyncio.gather(*tasks, return_exceptions=True)

            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()

            # åˆ¤æ–­æ˜¯å¦é€šè¿‡W4éªŒæ”¶
            w4_passed = self._evaluate_w4_success(final_report)

            return w4_passed

        except Exception as e:
            self.logger.error(f"âŒ W4å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            return False
        finally:
            await self._cleanup()

    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        self.logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·")
        self.test_running = False

    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        runtime_hours = (time.time() - self.start_time) / 3600

        # ç»Ÿè®¡å‘Šè­¦
        critical_alerts = [
            a
            for a in self.alerts
            if any(v.get("severity") == "critical" for v in a.get("violations", []))
        ]
        warning_alerts = [a for a in self.alerts if a not in critical_alerts]

        # æ€§èƒ½ç»Ÿè®¡
        if self.performance_samples:
            latencies = [s["cycle_duration_ms"] for s in self.performance_samples]
            latencies.sort()

            performance_stats = {
                "total_operations": len(self.performance_samples) * len(self.pairs),
                "avg_latency_ms": sum(latencies) / len(latencies),
                "p95_latency_ms": latencies[int(len(latencies) * 0.95)],
                "p99_latency_ms": latencies[int(len(latencies) * 0.99)],
                "max_latency_ms": max(latencies),
                "operations_per_second": len(self.performance_samples)
                * len(self.pairs)
                / (runtime_hours * 3600),
            }
        else:
            performance_stats = {}

        # æœ€ç»ˆç³»ç»ŸçŠ¶æ€
        final_metrics = await self._collect_system_metrics()

        return {
            "test_summary": {
                "duration_hours": runtime_hours,
                "target_hours": self.test_duration_hours,
                "completed": runtime_hours >= self.test_duration_hours,
                "pairs_tested": self.pairs,
                "frequency_hz": self.frequency_hz,
            },
            "alerts": {
                "total_alerts": len(self.alerts),
                "critical_alerts": len(critical_alerts),
                "warning_alerts": len(warning_alerts),
                "alert_details": self.alerts,
            },
            "performance": performance_stats,
            "final_metrics": final_metrics,
            "hourly_reports": self.hourly_reports,
            "w4_thresholds": self.w4_thresholds,
        }

    def _evaluate_w4_success(self, report: Dict[str, Any]) -> bool:
        """è¯„ä¼°W4éªŒæ”¶æ˜¯å¦æˆåŠŸ"""
        # W4æˆåŠŸæ ‡å‡†
        criteria = {
            "duration_completed": report["test_summary"]["completed"],
            "no_critical_alerts": report["alerts"]["critical_alerts"] == 0,
            "latency_acceptable": (
                report["performance"].get("p95_latency_ms", 0)
                <= self.w4_thresholds["max_latency_p95_ms"]
            ),
            "final_rss_acceptable": (
                report["final_metrics"]["memory"]["rss_mb"] <= self.w4_thresholds["max_rss_mb"]
            ),
        }

        # æ‰€æœ‰æ ‡å‡†éƒ½å¿…é¡»æ»¡è¶³
        all_passed = all(criteria.values())

        self.logger.info("ğŸ“‹ W4éªŒæ”¶è¯„ä¼°:")
        for criterion, passed in criteria.items():
            status = "âœ…" if passed else "âŒ"
            self.logger.info(f"   {status} {criterion}: {passed}")

        return all_passed

    async def _cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.logger.info("ğŸ§¹ æ¸…ç†W4æµ‹è¯•ç¯å¢ƒ...")

        # ç§»é™¤GCå›è°ƒ
        self.gc_optimizer.remove_gc_callbacks()

        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        if hasattr(self, "start_time") and self.start_time:
            final_report = await self._generate_final_report()

            timestamp = int(time.time())
            os.makedirs("output", exist_ok=True)

            with open(f"output/w4_stress_canary_{timestamp}.json", "w") as f:
                json.dump(final_report, f, indent=2, default=str)

        self.logger.info("âœ… W4æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="W4 24å°æ—¶å‹åŠ›Canaryæµ‹è¯•")
    parser.add_argument(
        "--pairs", nargs="+", default=["BTCUSDT", "ETHUSDT", "XRPUSDT"], help="äº¤æ˜“å¯¹åˆ—è¡¨"
    )
    parser.add_argument("--freq", type=int, default=5, help="æµ‹è¯•é¢‘ç‡(Hz)")
    parser.add_argument("--duration", type=int, default=24, help="æµ‹è¯•æ—¶é•¿(å°æ—¶)")
    parser.add_argument("--dry-run", action="store_true", help="çŸ­æ—¶é—´æµ‹è¯•è¿è¡Œ")

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.FileHandler("w4_stress_canary.log"), logging.StreamHandler()],
    )

    print("ğŸš€ W4 24å°æ—¶å‹åŠ›Canaryæµ‹è¯•å¯åŠ¨")
    print(f"äº¤æ˜“å¯¹: {args.pairs}")
    print(f"é¢‘ç‡: {args.freq}Hz")
    print(f"æ—¶é•¿: {args.duration}å°æ—¶")

    # åˆ›å»ºæµ‹è¯•å™¨
    canary = W4StressCanary(pairs=args.pairs, frequency_hz=args.freq)

    # è°ƒæ•´æµ‹è¯•æ—¶é•¿
    if args.dry_run:
        canary.test_duration_hours = 0.1  # 6åˆ†é’Ÿæµ‹è¯•
        print("ğŸ§ª å¹²è¿è¡Œæ¨¡å¼ï¼š6åˆ†é’Ÿæµ‹è¯•")
    else:
        canary.test_duration_hours = args.duration

    try:
        # è¿è¡Œå‹åŠ›æµ‹è¯•
        success = await canary.run_stress_test()

        if success:
            print("\nğŸ‰ W4å‹åŠ›æµ‹è¯•éªŒæ”¶é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥è¿›å…¥ä¸»ç½‘ç°åº¦")
            return True
        else:
            print("\nâš ï¸ W4å‹åŠ›æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            return False

    except Exception as e:
        print(f"âŒ W4å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
