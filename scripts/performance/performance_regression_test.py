#!/usr/bin/env python3
"""
M3é˜¶æ®µæ€§èƒ½å›å½’æµ‹è¯•
Performance Regression Test for M3 Phase

ç”¨é€”ï¼š
- PRä¸­è‡ªåŠ¨è¿è¡Œæ€§èƒ½æµ‹è¯•
- æ£€æµ‹æ€§èƒ½å›å½’ï¼ˆ>10%çš„å»¶è¿Ÿå¢åŠ ï¼‰
- ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
"""

import json
import time
import sys
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path

from scripts.performance_profiler import PerformanceProfiler


class PerformanceRegressionTest:
    """æ€§èƒ½å›å½’æµ‹è¯•å™¨"""
    
    def __init__(self, baseline_file: str = "output/m3_performance_baseline.json"):
        self.baseline_file = baseline_file
        self.baseline = self._load_baseline()
        self.profiler = PerformanceProfiler()
        
    def _load_baseline(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ€§èƒ½åŸºçº¿"""
        try:
            if Path(self.baseline_file).exists():
                with open(self.baseline_file, 'r') as f:
                    baseline = json.load(f)
                print(f"âœ… åŠ è½½åŸºçº¿: P95={baseline['latency_stats']['p95_ms']:.1f}ms")
                return baseline
            else:
                print(f"âš ï¸  åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {self.baseline_file}")
                return None
        except Exception as e:
            print(f"âŒ åŠ è½½åŸºçº¿å¤±è´¥: {e}")
            return None
    
    def run_regression_test(self, test_duration: int = 120) -> Dict[str, Any]:
        """
        è¿è¡Œæ€§èƒ½å›å½’æµ‹è¯•
        
        Args:
            test_duration: æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼‰
            
        Returns:
            æµ‹è¯•ç»“æœå’Œå¯¹æ¯”åˆ†æ
        """
        print(f"ğŸ§ª å¼€å§‹æ€§èƒ½å›å½’æµ‹è¯• - {test_duration}ç§’")
        
        # è¿è¡Œå½“å‰ç‰ˆæœ¬çš„æ€§èƒ½æµ‹è¯•
        current_results = self.profiler.dry_run_trading_loop(test_duration)
        
        if not self.baseline:
            print("âš ï¸  æ²¡æœ‰åŸºçº¿æ•°æ®ï¼Œä»…è®°å½•å½“å‰æ€§èƒ½")
            return {
                "status": "no_baseline",
                "current": current_results,
                "regression_detected": False
            }
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        comparison = self._compare_performance(current_results)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "status": "completed",
            "baseline": self.baseline,
            "current": current_results,
            "comparison": comparison,
            "regression_detected": comparison["regression_detected"]
        }
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = f"output/regression_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        # è¾“å‡ºç»“æœ
        self._print_regression_report(comparison)
        
        return report
    
    def _compare_performance(self, current: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æ€§èƒ½æ•°æ®"""
        baseline_p95 = self.baseline["latency_stats"]["p95_ms"]
        current_p95 = current["latency_stats"]["p95_ms"]
        
        baseline_throughput = self.baseline["throughput_cps"]
        current_throughput = current["throughput_cps"]
        
        # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
        p95_change = ((current_p95 - baseline_p95) / baseline_p95) * 100
        throughput_change = ((current_throughput - baseline_throughput) / baseline_throughput) * 100
        
        # åˆ¤æ–­æ˜¯å¦å›å½’
        regression_detected = False
        issues = []
        
        # P95å»¶è¿Ÿå›å½’æ£€æµ‹ï¼ˆè¶…è¿‡10%è®¤ä¸ºå›å½’ï¼‰
        if p95_change > 10:
            regression_detected = True
            issues.append(f"P95å»¶è¿Ÿå¢åŠ  {p95_change:.1f}%")
        
        # ååé‡å›å½’æ£€æµ‹ï¼ˆä¸‹é™è¶…è¿‡15%è®¤ä¸ºå›å½’ï¼‰
        if throughput_change < -15:
            regression_detected = True
            issues.append(f"ååé‡ä¸‹é™ {abs(throughput_change):.1f}%")
        
        return {
            "regression_detected": regression_detected,
            "issues": issues,
            "metrics": {
                "p95_latency": {
                    "baseline_ms": baseline_p95,
                    "current_ms": current_p95,
                    "change_percent": p95_change
                },
                "throughput": {
                    "baseline_cps": baseline_throughput,
                    "current_cps": current_throughput,
                    "change_percent": throughput_change
                }
            }
        }
    
    def _print_regression_report(self, comparison: Dict[str, Any]):
        """æ‰“å°å›å½’æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ¯ M3æ€§èƒ½å›å½’æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        metrics = comparison["metrics"]
        
        # P95å»¶è¿Ÿå¯¹æ¯”
        p95_baseline = metrics["p95_latency"]["baseline_ms"]
        p95_current = metrics["p95_latency"]["current_ms"]
        p95_change = metrics["p95_latency"]["change_percent"]
        
        print(f"ğŸ“Š P95å»¶è¿Ÿå¯¹æ¯”:")
        print(f"   åŸºçº¿:   {p95_baseline:.1f}ms")
        print(f"   å½“å‰:   {p95_current:.1f}ms")
        print(f"   å˜åŒ–:   {p95_change:+.1f}% {'âŒ' if p95_change > 10 else 'âœ…'}")
        
        # ååé‡å¯¹æ¯”
        tp_baseline = metrics["throughput"]["baseline_cps"]
        tp_current = metrics["throughput"]["current_cps"]
        tp_change = metrics["throughput"]["change_percent"]
        
        print(f"\nğŸš€ ååé‡å¯¹æ¯”:")
        print(f"   åŸºçº¿:   {tp_baseline:.2f} cycles/sec")
        print(f"   å½“å‰:   {tp_current:.2f} cycles/sec")
        print(f"   å˜åŒ–:   {tp_change:+.1f}% {'âŒ' if tp_change < -15 else 'âœ…'}")
        
        # å›å½’æ£€æµ‹ç»“æœ
        print(f"\nğŸ” å›å½’æ£€æµ‹: {'âŒ æ£€æµ‹åˆ°å›å½’' if comparison['regression_detected'] else 'âœ… æ— å›å½’'}")
        
        if comparison["issues"]:
            print("âš ï¸  å‘ç°çš„é—®é¢˜:")
            for issue in comparison["issues"]:
                print(f"   - {issue}")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•° - æ€§èƒ½å›å½’æµ‹è¯•å…¥å£"""
    if len(sys.argv) > 1:
        test_duration = int(sys.argv[1])
    else:
        test_duration = 120  # é»˜è®¤2åˆ†é’Ÿ
    
    print("ğŸ§ª M3æ€§èƒ½å›å½’æµ‹è¯•å¯åŠ¨")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = PerformanceRegressionTest()
    
    # è¿è¡Œå›å½’æµ‹è¯•
    report = tester.run_regression_test(test_duration)
    
    # é€€å‡ºç ï¼šæœ‰å›å½’åˆ™è¿”å›1ï¼Œæ— å›å½’è¿”å›0
    exit_code = 1 if report.get("regression_detected", False) else 0
    
    if exit_code == 1:
        print("\nâŒ æ€§èƒ½å›å½’æ£€æµ‹ï¼šæµ‹è¯•å¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ä»£ç å˜æ›´ï¼Œä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ")
    else:
        print("\nâœ… æ€§èƒ½å›å½’æ£€æµ‹ï¼šæµ‹è¯•é€šè¿‡")
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main() 