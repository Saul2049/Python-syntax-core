#!/usr/bin/env python3
"""
æ•°æ®ä¿å­˜å™¨å®Œæ•´æµ‹è¯•å¥—ä»¶ (Data Saver Comprehensive Tests)

ä¸“é—¨æå‡src/data/validators/data_saver.pyè¦†ç›–ç‡
ç›®æ ‡ï¼šä»30%è¦†ç›–ç‡æå‡åˆ°80%+
"""

import json
import os
import tempfile
from datetime import datetime
from pathlib import Path
from unittest.mock import patch

import numpy as np
import pandas as pd
import pytest

from src.data.validators.data_saver import DataSaver, ProcessedDataExporter, save_processed_data


class TestDataSaverCore:
    """æ•°æ®ä¿å­˜å™¨æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•"""

    @pytest.fixture
    def temp_dir(self):
        """ä¸´æ—¶ç›®å½•fixture"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir

    @pytest.fixture
    def sample_dataframe(self):
        """ç¤ºä¾‹DataFrame"""
        return pd.DataFrame(
            {
                "timestamp": pd.date_range("2023-01-01", periods=100, freq="h"),
                "price": np.random.rand(100) * 100 + 50,
                "volume": np.random.randint(1000, 10000, 100),
                "signal": np.random.choice(["buy", "sell", "hold"], 100),
                "value": np.random.randn(100),
            }
        )

    def test_init_default_dir(self):
        """æµ‹è¯•é»˜è®¤ç›®å½•åˆå§‹åŒ–"""
        with patch("pathlib.Path.mkdir") as mock_mkdir:
            saver = DataSaver()
            assert saver.base_output_dir == Path("output")
            mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)

    def test_init_custom_dir(self, temp_dir):
        """æµ‹è¯•è‡ªå®šä¹‰ç›®å½•åˆå§‹åŒ–"""
        custom_dir = Path(temp_dir) / "custom"
        saver = DataSaver(base_output_dir=custom_dir)
        assert saver.base_output_dir == custom_dir
        assert custom_dir.exists()

    def test_init_string_path(self, temp_dir):
        """æµ‹è¯•å­—ç¬¦ä¸²è·¯å¾„åˆå§‹åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        assert saver.base_output_dir == Path(temp_dir)

    def test_save_data_csv_success(self, temp_dir, sample_dataframe):
        """æµ‹è¯•CSVæ ¼å¼ä¿å­˜æˆåŠŸ"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.csv", file_format="csv")

        assert result is True
        file_path = Path(temp_dir) / "test.csv"
        assert file_path.exists()

        # éªŒè¯æ–‡ä»¶å¯ä»¥åŠ è½½ï¼Œä½†ä¸è¿›è¡Œä¸¥æ ¼çš„ç±»å‹æ¯”è¾ƒ
        # å› ä¸ºCSVæ ¼å¼ä¼šæ”¹å˜æŸäº›æ•°æ®ç±»å‹ï¼ˆå¦‚datetimeå˜ä¸ºobjectï¼‰
        loaded_df = pd.read_csv(file_path, index_col=0)
        assert len(loaded_df) == len(sample_dataframe)
        assert list(loaded_df.columns) == list(sample_dataframe.columns)

    def test_save_data_with_subdirectory(self, temp_dir, sample_dataframe):
        """æµ‹è¯•åœ¨å­ç›®å½•ä¸­ä¿å­˜æ•°æ®"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.csv", subdirectory="data/raw")

        assert result is True
        file_path = Path(temp_dir) / "data" / "raw" / "test.csv"
        assert file_path.exists()

    def test_save_data_with_timestamp(self, temp_dir, sample_dataframe):
        """æµ‹è¯•æ·»åŠ æ—¶é—´æˆ³çš„ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)

        with patch("src.data.validators.data_saver.datetime") as mock_datetime:
            mock_datetime.now.return_value.strftime.return_value = "20231201_143022"
            result = saver.save_data(sample_dataframe, "test.csv", add_timestamp=True)

        assert result is True
        file_path = Path(temp_dir) / "test_20231201_143022.csv"
        assert file_path.exists()

    def test_save_data_parquet_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•Parquetæ ¼å¼ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.parquet", file_format="parquet")

        assert result is True
        file_path = Path(temp_dir) / "test.parquet"
        assert file_path.exists()

    def test_save_data_json_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•JSONæ ¼å¼ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.json", file_format="json")

        assert result is True
        file_path = Path(temp_dir) / "test.json"
        assert file_path.exists()

    def test_save_data_excel_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•Excelæ ¼å¼ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.xlsx", file_format="excel")

        assert result is True
        file_path = Path(temp_dir) / "test.xlsx"
        assert file_path.exists()

    def test_save_data_pickle_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•Pickleæ ¼å¼ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.pkl", file_format="pickle")

        assert result is True
        file_path = Path(temp_dir) / "test.pkl"
        assert file_path.exists()

    def test_save_data_hdf5_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•HDF5æ ¼å¼ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.h5", file_format="hdf5", key="test_data")

        assert result is True
        file_path = Path(temp_dir) / "test.h5"
        assert file_path.exists()

    def test_save_data_unsupported_format(self, temp_dir, sample_dataframe):
        """æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.xyz", file_format="xyz")

        assert result is False

    def test_save_data_exception_handling(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†"""
        saver = DataSaver(base_output_dir=temp_dir)

        # æ¨¡æ‹Ÿä¿å­˜å¼‚å¸¸
        with patch.object(pd.DataFrame, "to_csv", side_effect=Exception("Test error")):
            result = saver.save_data(sample_dataframe, "test.csv")
            assert result is False

    def test_save_metadata_enabled(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å…ƒæ•°æ®ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.csv", save_metadata=True)

        assert result is True
        metadata_path = Path(temp_dir) / "test.metadata.json"
        assert metadata_path.exists()

        # éªŒè¯å…ƒæ•°æ®å†…å®¹
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)

        assert "filename" in metadata
        assert "created_at" in metadata
        assert "shape" in metadata
        assert "columns" in metadata
        assert metadata["shape"] == list(sample_dataframe.shape)

    def test_save_metadata_disabled(self, temp_dir, sample_dataframe):
        """æµ‹è¯•ç¦ç”¨å…ƒæ•°æ®ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.csv", save_metadata=False)

        assert result is True
        metadata_path = Path(temp_dir) / "test.metadata.json"
        assert not metadata_path.exists()

    def test_create_backup_functionality(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å¤‡ä»½åŠŸèƒ½"""
        saver = DataSaver(base_output_dir=temp_dir)
        file_path = Path(temp_dir) / "test.csv"

        # å…ˆä¿å­˜ä¸€ä¸ªæ–‡ä»¶
        sample_dataframe.to_csv(file_path)

        # å†æ¬¡ä¿å­˜å¹¶åˆ›å»ºå¤‡ä»½
        with patch("src.data.validators.data_saver.datetime") as mock_datetime:
            mock_datetime.now.return_value.strftime.return_value = "20231201_143022"
            result = saver.save_data(sample_dataframe, "test.csv", create_backup=True)

        assert result is True
        backup_dir = Path(temp_dir) / "backups"
        assert backup_dir.exists()
        backup_file = backup_dir / "test_20231201_143022.csv"
        assert backup_file.exists()

    def test_create_backup_no_original_file(self, temp_dir, sample_dataframe):
        """æµ‹è¯•åŸæ–‡ä»¶ä¸å­˜åœ¨æ—¶çš„å¤‡ä»½"""
        saver = DataSaver(base_output_dir=temp_dir)
        result = saver.save_data(sample_dataframe, "test.csv", create_backup=True)

        assert result is True
        # ä¸åº”è¯¥åˆ›å»ºå¤‡ä»½ç›®å½•ï¼Œå› ä¸ºåŸæ–‡ä»¶ä¸å­˜åœ¨
        backup_dir = Path(temp_dir) / "backups"
        assert not backup_dir.exists()

    def test_create_backup_exception(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å¤‡ä»½åˆ›å»ºå¼‚å¸¸"""
        saver = DataSaver(base_output_dir=temp_dir)
        file_path = Path(temp_dir) / "test.csv"
        sample_dataframe.to_csv(file_path)

        with patch("shutil.copy2", side_effect=Exception("Backup error")):
            # åº”è¯¥ä»ç„¶æˆåŠŸä¿å­˜ï¼Œåªæ˜¯å¤‡ä»½å¤±è´¥
            result = saver.save_data(sample_dataframe, "test.csv", create_backup=True)
            assert result is True

    def test_save_metadata_exception(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å…ƒæ•°æ®ä¿å­˜å¼‚å¸¸"""
        saver = DataSaver(base_output_dir=temp_dir)

        # ä½¿ç”¨æ›´ç²¾ç¡®çš„patchæ¥åªå½±å“å…ƒæ•°æ®æ–‡ä»¶å†™å…¥
        original_open = open

        def mock_open_func(*args, **kwargs):
            if len(args) > 0 and "metadata.json" in str(args[0]):
                raise Exception("Metadata error")
            return original_open(*args, **kwargs)

        with patch("builtins.open", side_effect=mock_open_func):
            # åº”è¯¥ä»ç„¶æˆåŠŸä¿å­˜ä¸»æ–‡ä»¶
            result = saver.save_data(sample_dataframe, "test.csv", save_metadata=True)
            assert result is True

    def test_save_multiple_formats_success(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å¤šæ ¼å¼ä¿å­˜æˆåŠŸ"""
        saver = DataSaver(base_output_dir=temp_dir)
        formats = ["csv", "json", "pickle"]
        results = saver.save_multiple_formats(sample_dataframe, "test", formats)

        assert isinstance(results, dict)
        assert len(results) == 3
        for fmt in formats:
            assert results[fmt] is True
            if fmt == "excel":
                file_path = Path(temp_dir) / "test.xlsx"
            else:
                file_path = Path(temp_dir) / f"test.{fmt}"
            assert file_path.exists()

    def test_save_multiple_formats_partial_failure(self, temp_dir, sample_dataframe):
        """æµ‹è¯•å¤šæ ¼å¼ä¿å­˜éƒ¨åˆ†å¤±è´¥"""
        saver = DataSaver(base_output_dir=temp_dir)
        formats = ["csv", "unsupported", "json"]
        results = saver.save_multiple_formats(sample_dataframe, "test", formats)

        assert results["csv"] is True
        assert results["unsupported"] is False
        assert results["json"] is True

    def test_batch_save_success(self, temp_dir):
        """æµ‹è¯•æ‰¹é‡ä¿å­˜æˆåŠŸ"""
        saver = DataSaver(base_output_dir=temp_dir)

        dataframes = {
            "data1": pd.DataFrame({"A": [1, 2, 3]}),
            "data2": pd.DataFrame({"B": [4, 5, 6]}),
            "data3": pd.DataFrame({"C": [7, 8, 9]}),
        }

        results = saver.batch_save(dataframes, file_format="csv")

        assert len(results) == 3
        # batch_save è¿”å›çš„keyåŒ…å«æ‰©å±•å
        for name in dataframes.keys():
            filename_with_ext = f"{name}.csv"
            assert results[filename_with_ext] is True
            file_path = Path(temp_dir) / f"{name}.csv"
            assert file_path.exists()

    def test_batch_save_with_subdirectory(self, temp_dir):
        """æµ‹è¯•åœ¨å­ç›®å½•ä¸­æ‰¹é‡ä¿å­˜"""
        saver = DataSaver(base_output_dir=temp_dir)

        dataframes = {"batch1": pd.DataFrame({"X": [1, 2]}), "batch2": pd.DataFrame({"Y": [3, 4]})}

        results = saver.batch_save(dataframes, subdirectory="batch_data")

        for name in dataframes.keys():
            filename_with_ext = f"{name}.csv"
            assert results[filename_with_ext] is True
            file_path = Path(temp_dir) / "batch_data" / f"{name}.csv"
            assert file_path.exists()

    def test_get_saved_files_info_basic(self, temp_dir, sample_dataframe):
        """æµ‹è¯•è·å–ä¿å­˜æ–‡ä»¶ä¿¡æ¯"""
        saver = DataSaver(base_output_dir=temp_dir)

        # ä¿å­˜å‡ ä¸ªæ–‡ä»¶
        saver.save_data(sample_dataframe, "file1.csv")
        saver.save_data(sample_dataframe, "file2.json", file_format="json")

        files_info = saver.get_saved_files_info()

        assert isinstance(files_info, list)
        assert len(files_info) >= 2

        # éªŒè¯æ–‡ä»¶ä¿¡æ¯ç»“æ„ - æ ¹æ®å®é™…å®ç°è°ƒæ•´
        for info in files_info:
            assert "filename" in info
            assert "size_mb" in info
            assert "created" in info  # å®é™…APIä½¿ç”¨'created'è€Œä¸æ˜¯'created_at'
            assert "path" in info  # å®é™…APIä½¿ç”¨'path'è€Œä¸æ˜¯'file_path'

    def test_get_saved_files_info_with_subdirectory(self, temp_dir, sample_dataframe):
        """æµ‹è¯•è·å–å­ç›®å½•æ–‡ä»¶ä¿¡æ¯"""
        saver = DataSaver(base_output_dir=temp_dir)

        # åœ¨å­ç›®å½•ä¸­ä¿å­˜æ–‡ä»¶
        saver.save_data(sample_dataframe, "subfile.csv", subdirectory="subdir")

        files_info = saver.get_saved_files_info(subdirectory="subdir")

        assert isinstance(files_info, list)
        assert len(files_info) >= 1

    def test_get_saved_files_info_empty_directory(self, temp_dir):
        """æµ‹è¯•ç©ºç›®å½•çš„æ–‡ä»¶ä¿¡æ¯"""
        saver = DataSaver(base_output_dir=temp_dir)
        files_info = saver.get_saved_files_info()

        assert isinstance(files_info, list)
        assert len(files_info) == 0

    def test_cleanup_old_files_basic(self, temp_dir, sample_dataframe):
        """æµ‹è¯•æ¸…ç†æ—§æ–‡ä»¶åŸºç¡€åŠŸèƒ½"""
        saver = DataSaver(base_output_dir=temp_dir)

        # ä¿å­˜ä¸€ä¸ªæ–‡ä»¶
        file_path = Path(temp_dir) / "old_file.csv"
        saver.save_data(sample_dataframe, "old_file.csv")

        # ä¿®æ”¹æ–‡ä»¶æ—¶é—´ä¸º31å¤©å‰
        old_time = datetime.now().timestamp() - (31 * 24 * 3600)
        os.utime(file_path, (old_time, old_time))

        # æ¸…ç†30å¤©å‰çš„æ–‡ä»¶
        deleted_count = saver.cleanup_old_files(days_old=30)

        assert deleted_count >= 1
        assert not file_path.exists()

    def test_cleanup_old_files_pattern(self, temp_dir, sample_dataframe):
        """æµ‹è¯•æŒ‰æ¨¡å¼æ¸…ç†æ–‡ä»¶"""
        saver = DataSaver(base_output_dir=temp_dir)

        # ä¿å­˜ä¸åŒç±»å‹çš„æ–‡ä»¶
        csv_file = Path(temp_dir) / "data.csv"
        json_file = Path(temp_dir) / "data.json"

        saver.save_data(sample_dataframe, "data.csv")
        saver.save_data(sample_dataframe, "data.json", file_format="json")

        # è®¾ç½®ä¸ºæ—§æ–‡ä»¶
        old_time = datetime.now().timestamp() - (31 * 24 * 3600)
        os.utime(csv_file, (old_time, old_time))
        os.utime(json_file, (old_time, old_time))

        # åªæ¸…ç†CSVæ–‡ä»¶
        deleted_count = saver.cleanup_old_files(days_old=30, file_pattern="*.csv")

        assert deleted_count >= 1
        assert not csv_file.exists()
        assert json_file.exists()  # JSONæ–‡ä»¶åº”è¯¥ä¿ç•™

    def test_cleanup_old_files_with_subdirectory(self, temp_dir, sample_dataframe):
        """æµ‹è¯•æ¸…ç†å­ç›®å½•ä¸­çš„æ—§æ–‡ä»¶"""
        saver = DataSaver(base_output_dir=temp_dir)

        # åœ¨å­ç›®å½•ä¸­ä¿å­˜æ–‡ä»¶
        saver.save_data(sample_dataframe, "subfile.csv", subdirectory="cleanup_test")
        file_path = Path(temp_dir) / "cleanup_test" / "subfile.csv"

        # è®¾ç½®ä¸ºæ—§æ–‡ä»¶
        old_time = datetime.now().timestamp() - (31 * 24 * 3600)
        os.utime(file_path, (old_time, old_time))

        deleted_count = saver.cleanup_old_files(subdirectory="cleanup_test", days_old=30)

        assert deleted_count >= 1
        assert not file_path.exists()

    def test_cleanup_old_files_no_files(self, temp_dir):
        """æµ‹è¯•æ¸…ç†æ— æ–‡ä»¶çš„æƒ…å†µ"""
        saver = DataSaver(base_output_dir=temp_dir)
        deleted_count = saver.cleanup_old_files(days_old=30)

        assert deleted_count == 0

    def test_cleanup_with_metadata_files(self, temp_dir, sample_dataframe):
        """æµ‹è¯•æ¸…ç†åŒ…å«å…ƒæ•°æ®çš„æ–‡ä»¶"""
        saver = DataSaver(base_output_dir=temp_dir)

        # ä¿å­˜å¸¦å…ƒæ•°æ®çš„æ–‡ä»¶
        result = saver.save_data(sample_dataframe, "meta_test.csv", save_metadata=True)

        assert result is True  # ç¡®ä¿ä¿å­˜æˆåŠŸ

        csv_file = Path(temp_dir) / "meta_test.csv"
        metadata_file = Path(temp_dir) / "meta_test.metadata.json"

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        assert csv_file.exists()

        # è®¾ç½®ä¸ºæ—§æ–‡ä»¶
        old_time = datetime.now().timestamp() - (31 * 24 * 3600)
        os.utime(csv_file, (old_time, old_time))
        if metadata_file.exists():
            os.utime(metadata_file, (old_time, old_time))

        deleted_count = saver.cleanup_old_files(days_old=30)

        assert deleted_count >= 1
        assert not csv_file.exists()
        if metadata_file.exists():
            assert not metadata_file.exists()


class TestProcessedDataExporter:
    """å¤„ç†æ•°æ®å¯¼å‡ºå™¨æµ‹è¯•"""

    @pytest.fixture
    def temp_dir(self):
        """ä¸´æ—¶ç›®å½•fixture"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir

    @pytest.fixture
    def ohlcv_data(self):
        """OHLCVæ•°æ®fixture"""
        return pd.DataFrame(
            {
                "timestamp": pd.date_range("2023-01-01", periods=100, freq="h"),
                "open": np.random.rand(100) * 100 + 50,
                "high": np.random.rand(100) * 100 + 55,
                "low": np.random.rand(100) * 100 + 45,
                "close": np.random.rand(100) * 100 + 50,
                "volume": np.random.randint(1000, 10000, 100),
                "sma_20": np.random.rand(100) * 100 + 50,
                "rsi": np.random.rand(100) * 100,
            }
        )

    def test_exporter_init_default(self):
        """æµ‹è¯•å¯¼å‡ºå™¨é»˜è®¤åˆå§‹åŒ–"""
        exporter = ProcessedDataExporter()
        assert isinstance(exporter.data_saver, DataSaver)

    def test_exporter_init_custom_saver(self, temp_dir):
        """æµ‹è¯•å¯¼å‡ºå™¨è‡ªå®šä¹‰ä¿å­˜å™¨åˆå§‹åŒ–"""
        custom_saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=custom_saver)
        assert exporter.data_saver is custom_saver

    def test_export_ohlcv_data_basic(self, temp_dir, ohlcv_data):
        """æµ‹è¯•OHLCVæ•°æ®å¯¼å‡ºåŸºç¡€åŠŸèƒ½"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        result = exporter.export_ohlcv_data(ohlcv_data, "BTCUSDT", "1h")

        assert result is True
        # æ ¹æ®å®é™…APIï¼Œæ–‡ä»¶ä¿å­˜åœ¨ohlcv_dataç›®å½•ä¸­
        expected_dir = Path(temp_dir) / "ohlcv_data"
        assert expected_dir.exists()
        # æ–‡ä»¶åä¼šåŒ…å«æ—¶é—´æˆ³
        files = list(expected_dir.glob("BTCUSDT_1h_ohlcv_with_indicators_*.csv"))
        assert len(files) >= 1

    def test_export_ohlcv_data_without_indicators(self, temp_dir, ohlcv_data):
        """æµ‹è¯•ä¸åŒ…å«æŒ‡æ ‡çš„OHLCVæ•°æ®å¯¼å‡º"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        result = exporter.export_ohlcv_data(ohlcv_data, "ETHUSDT", "4h", include_indicators=False)

        assert result is True
        expected_dir = Path(temp_dir) / "ohlcv_data"
        assert expected_dir.exists()
        files = list(expected_dir.glob("ETHUSDT_4h_ohlcv_*.csv"))
        assert len(files) >= 1

    def test_export_signals_data(self, temp_dir):
        """æµ‹è¯•ä¿¡å·æ•°æ®å¯¼å‡º"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        signals_data = pd.DataFrame(
            {
                "timestamp": pd.date_range("2023-01-01", periods=50, freq="h"),
                "signal": np.random.choice(["buy", "sell", "hold"], 50),
                "confidence": np.random.rand(50),
                "price": np.random.rand(50) * 100 + 50,
            }
        )

        result = exporter.export_signals_data(signals_data, "momentum_strategy")

        assert result is True
        # æ ¹æ®å®é™…APIï¼Œæ–‡ä»¶ä¿å­˜åœ¨signalsç›®å½•ä¸­
        expected_dir = Path(temp_dir) / "signals"
        assert expected_dir.exists()
        files = list(expected_dir.glob("momentum_strategy_signals_*.csv"))
        assert len(files) >= 1

    def test_export_backtest_results(self, temp_dir):
        """æµ‹è¯•å›æµ‹ç»“æœå¯¼å‡º"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        # åˆ›å»ºåŒ…å«å„ç§æ•°æ®ç±»å‹çš„å›æµ‹ç»“æœ
        backtest_results = {
            "strategy_name": "test_strategy",
            "total_return": 0.15,
            "sharpe_ratio": 1.25,
            "max_drawdown": 0.08,
            "win_rate": 0.65,
            "trades": [
                {"entry_time": "2023-01-01", "exit_time": "2023-01-02", "pnl": 100},
                {"entry_time": "2023-01-03", "exit_time": "2023-01-04", "pnl": -50},
            ],
            "equity_curve": pd.Series([100000, 105000, 110000, 108000]),
            "metadata": {
                "start_date": "2023-01-01",
                "end_date": "2023-12-31",
                "parameters": {"fast_ma": 10, "slow_ma": 20},
            },
        }

        result = exporter.export_backtest_results(backtest_results, "test_strategy")

        assert result is True
        # æ ¹æ®å®é™…APIï¼Œæ–‡ä»¶ä¿å­˜åœ¨backtest_resultsç›®å½•ä¸­
        expected_file = Path(temp_dir) / "backtest_results" / "test_strategy_backtest_results.json"
        assert expected_file.exists()

        # éªŒè¯JSONå†…å®¹
        with open(expected_file, "r", encoding="utf-8") as f:
            saved_data = json.load(f)

        assert saved_data["strategy_name"] == "test_strategy"
        assert saved_data["total_return"] == 0.15

    def test_make_serializable_pandas_series(self, temp_dir):
        """æµ‹è¯•pandas Seriesåºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        series = pd.Series([1, 2, 3, 4, 5], name="test_series")
        result = exporter._make_serializable(series)

        # æ ¹æ®å®é™…å®ç°ï¼ŒSeriesè¢«è½¬æ¢ä¸ºdictè€Œä¸æ˜¯list
        assert isinstance(result, dict)

    def test_make_serializable_pandas_dataframe(self, temp_dir):
        """æµ‹è¯•pandas DataFrameåºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        df = pd.DataFrame({"A": [1, 2], "B": [3, 4]})
        result = exporter._make_serializable(df)

        assert isinstance(result, dict)
        assert "A" in result
        assert "B" in result

    def test_make_serializable_numpy_scalar(self, temp_dir):
        """æµ‹è¯•numpyæ ‡é‡åºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        scalar = np.float64(3.14)
        result = exporter._make_serializable(scalar)

        # numpy.float64å®é™…ä¸Šè¢«è®¤ä¸ºæ˜¯åŸºæœ¬ç±»å‹(float)ï¼Œæ‰€ä»¥è¿”å›åŸå€¼
        assert isinstance(result, (float, np.float64))
        assert result == scalar

    def test_make_serializable_dict(self, temp_dir):
        """æµ‹è¯•å­—å…¸é€’å½’åºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        data = {"series": pd.Series([1, 2, 3]), "nested": {"scalar": np.int32(42)}}

        result = exporter._make_serializable(data)

        assert isinstance(result, dict)
        assert isinstance(result["series"], dict)  # Seriesè½¬ä¸ºdict
        assert isinstance(result["nested"]["scalar"], str)  # numpyæ ‡é‡è½¬ä¸ºstr

    def test_make_serializable_list(self, temp_dir):
        """æµ‹è¯•åˆ—è¡¨é€’å½’åºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        data = [pd.Series([1, 2]), np.float32(5.5)]

        result = exporter._make_serializable(data)

        assert isinstance(result, list)
        assert isinstance(result[0], dict)  # Seriesè½¬ä¸ºdict
        assert isinstance(result[1], str)  # numpyæ ‡é‡è½¬ä¸ºstr

    def test_make_serializable_primitive_types(self, temp_dir):
        """æµ‹è¯•åŸºæœ¬ç±»å‹åºåˆ—åŒ–"""
        saver = DataSaver(base_output_dir=temp_dir)
        exporter = ProcessedDataExporter(data_saver=saver)

        # æµ‹è¯•å·²ç»å¯åºåˆ—åŒ–çš„ç±»å‹
        assert exporter._make_serializable(42) == 42
        assert exporter._make_serializable(3.14) == 3.14
        assert exporter._make_serializable("hello") == "hello"
        assert exporter._make_serializable(True) is True
        assert exporter._make_serializable(None) is None


class TestSaveProcessedDataFunction:
    """save_processed_dataå‡½æ•°æµ‹è¯•"""

    @pytest.fixture
    def temp_dir(self):
        """ä¸´æ—¶ç›®å½•fixture"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir

    @pytest.fixture
    def sample_data(self):
        """ç¤ºä¾‹æ•°æ®"""
        return pd.DataFrame({"value1": [1, 2, 3, 4, 5], "value2": [10, 20, 30, 40, 50]})

    def test_save_processed_data_csv(self, temp_dir, sample_data):
        """æµ‹è¯•CSVæ ¼å¼ä¿å­˜"""
        output_path = os.path.join(temp_dir, "processed_data.csv")
        result = save_processed_data(sample_data, output_path, file_format="csv")

        assert result is True
        assert os.path.exists(output_path)

    def test_save_processed_data_pickle(self, temp_dir, sample_data):
        """æµ‹è¯•Pickleæ ¼å¼ä¿å­˜"""
        output_path = os.path.join(temp_dir, "processed_data.pkl")
        result = save_processed_data(sample_data, output_path, file_format="pickle")

        assert result is True
        assert os.path.exists(output_path)

    def test_save_processed_data_with_kwargs(self, temp_dir, sample_data):
        """æµ‹è¯•å¸¦é¢å¤–å‚æ•°çš„ä¿å­˜"""
        output_path = os.path.join(temp_dir, "processed_data.csv")
        result = save_processed_data(
            sample_data, output_path, file_format="csv", index=False, sep=";"
        )

        assert result is True
        assert os.path.exists(output_path)

        # éªŒè¯å‚æ•°ç”Ÿæ•ˆ
        loaded_data = pd.read_csv(output_path, sep=";")
        assert len(loaded_data) == len(sample_data)

    def test_save_processed_data_exception(self, temp_dir, sample_data):
        """æµ‹è¯•ä¿å­˜å¼‚å¸¸å¤„ç†"""
        # ä½¿ç”¨æ— æ•ˆè·¯å¾„
        invalid_path = "/invalid/path/data.csv"
        result = save_processed_data(sample_data, invalid_path)

        assert result is False

    def test_save_processed_data_unsupported_format(self, temp_dir, sample_data):
        """æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼"""
        output_path = os.path.join(temp_dir, "data.xyz")
        result = save_processed_data(sample_data, output_path, file_format="xyz")

        assert result is False


class TestEdgeCasesAndErrorHandling:
    """è¾¹ç¼˜æƒ…å†µå’Œé”™è¯¯å¤„ç†æµ‹è¯•"""

    @pytest.fixture
    def temp_dir(self):
        """ä¸´æ—¶ç›®å½•fixture"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir

    def test_save_empty_dataframe(self, temp_dir):
        """æµ‹è¯•ä¿å­˜ç©ºDataFrame"""
        saver = DataSaver(base_output_dir=temp_dir)
        empty_df = pd.DataFrame()

        result = saver.save_data(empty_df, "empty.csv")
        assert result is True

        file_path = Path(temp_dir) / "empty.csv"
        assert file_path.exists()

    def test_save_large_dataframe(self, temp_dir):
        """æµ‹è¯•ä¿å­˜å¤§DataFrame"""
        saver = DataSaver(base_output_dir=temp_dir)

        # åˆ›å»ºè¾ƒå¤§çš„DataFrame
        large_df = pd.DataFrame({f"col_{i}": np.random.randn(1000) for i in range(50)})

        result = saver.save_data(large_df, "large.csv")
        assert result is True

    def test_unicode_in_data(self, temp_dir):
        """æµ‹è¯•æ•°æ®ä¸­çš„Unicodeå­—ç¬¦"""
        saver = DataSaver(base_output_dir=temp_dir)

        unicode_df = pd.DataFrame(
            {
                "chinese": ["ä½ å¥½", "ä¸–ç•Œ", "æµ‹è¯•"],
                "emoji": ["ğŸ˜€", "ğŸš€", "ğŸ’°"],
                "arabic": ["Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù„Ø¹Ø§Ù„Ù…", "Ø§Ø®ØªØ¨Ø§Ø±"],
            }
        )

        result = saver.save_data(unicode_df, "unicode_test.csv")
        assert result is True

        # éªŒè¯Unicodeæ•°æ®ä¿å­˜æ­£ç¡®
        file_path = Path(temp_dir) / "unicode_test.csv"
        loaded_df = pd.read_csv(file_path, index_col=0)
        assert "ä½ å¥½" in loaded_df["chinese"].values

    def test_dataframe_with_complex_dtypes(self, temp_dir):
        """æµ‹è¯•å¤æ‚æ•°æ®ç±»å‹çš„DataFrame"""
        saver = DataSaver(base_output_dir=temp_dir)

        complex_df = pd.DataFrame(
            {
                "integers": pd.array([1, 2, 3], dtype="Int64"),
                "floats": pd.array([1.1, 2.2, None], dtype="Float64"),
                "strings": pd.array(["a", "b", None], dtype="string"),
                "booleans": pd.array([True, False, None], dtype="boolean"),
                "categories": pd.Categorical(["X", "Y", "X"]),
                "datetime": pd.date_range("2023-01-01", periods=3),
            }
        )

        result = saver.save_data(complex_df, "complex_types.csv")
        assert result is True

    def test_concurrent_save_operations(self, temp_dir):
        """æµ‹è¯•å¹¶å‘ä¿å­˜æ“ä½œ"""
        import threading

        saver = DataSaver(base_output_dir=temp_dir)
        results = []

        def save_operation(i):
            df = pd.DataFrame({"data": [i, i + 1, i + 2]})
            result = saver.save_data(df, f"concurrent_{i}.csv")
            results.append(result)

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘ä¿å­˜
        threads = []
        for i in range(5):
            thread = threading.Thread(target=save_operation, args=(i,))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸ
        assert all(results)
        assert len(results) == 5
