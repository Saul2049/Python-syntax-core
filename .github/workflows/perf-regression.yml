name: 🚀 M4 Performance Regression Guard

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'requirements.txt'
  push:
    branches: [ main ]

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  performance-regression:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r dev-requirements.txt
        pip install -e .
        
    - name: 🧪 Verify Installation
      run: |
        python -c "from src.core.signal_processor_vectorized import OptimizedSignalProcessor; print('✅ Core modules OK')"
        python -c "from src.monitoring.metrics_collector import get_metrics_collector; print('✅ Monitoring OK')"
        
    - name: ⚡ Run M4 Performance Benchmark
      id: benchmark
      run: |
        echo "🚀 Running M4 Performance Benchmark (120s)..."
        timeout 300 python scripts/performance/m4_simple_benchmark.py || {
          echo "⚠️ Benchmark timeout or error, continuing with reduced test"
          echo "benchmark_failed=true" >> $GITHUB_OUTPUT
        }
        
    - name: 🎯 Assert P95 Performance Threshold
      run: |
        echo "🔍 Checking P95 latency threshold..."
        if [ "${{ steps.benchmark.outputs.benchmark_failed }}" = "true" ]; then
          echo "⚠️ Benchmark failed, running assertion with fallback"
          python scripts/health/assert_p95.py --p95-threshold 0.003 --quick
        else
          echo "✅ Running full assertion check"
          python scripts/health/assert_p95.py --p95-threshold 0.0025 --quick
        fi
        
    - name: 📊 Upload Performance Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ github.sha }}
        path: |
          output/m4_simple_benchmark_*.json
          output/p95_assertion_*.json
        if-no-files-found: warn
        retention-days: 30
        
    - name: 💬 Comment PR with Results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Find latest performance results
          const outputDir = 'output';
          let resultComment = '## 🚀 M4 Performance Regression Results\n\n';
          
          try {
            // Look for assertion results
            const files = fs.readdirSync(outputDir);
            const assertionFile = files.find(f => f.includes('p95_assertion_'));
            
            if (assertionFile) {
              const results = JSON.parse(fs.readFileSync(path.join(outputDir, assertionFile), 'utf8'));
              const checks = results.assertion_results.checks;
              
              resultComment += '### 🎯 Performance Assertions\n\n';
              resultComment += `| Metric | Value | Threshold | Status |\n`;
              resultComment += `|--------|--------|-----------|--------|\n`;
              
              // Async signals P95
              const asyncCheck = checks.async_signals_p95;
              const asyncStatus = asyncCheck.passed ? '✅ PASS' : '❌ FAIL';
              resultComment += `| Async Signals P95 | ${asyncCheck.value_ms.toFixed(1)}ms | ${asyncCheck.threshold_ms.toFixed(1)}ms | ${asyncStatus} |\n`;
              
              // Task scheduling P95
              const taskCheck = checks.task_scheduling_p95;
              const taskStatus = taskCheck.passed ? '✅ PASS' : '❌ FAIL';
              resultComment += `| Task Scheduling P95 | ${taskCheck.value_ms.toFixed(1)}ms | ${taskCheck.threshold_ms.toFixed(1)}ms | ${taskStatus} |\n`;
              
              // CPU usage
              const cpuCheck = checks.cpu_usage_max;
              const cpuStatus = cpuCheck.passed ? '✅ PASS' : '❌ FAIL';
              resultComment += `| CPU Usage Max | ${cpuCheck.value_percent.toFixed(1)}% | ${cpuCheck.threshold_percent}% | ${cpuStatus} |\n`;
              
              // Overall result
              if (results.assertion_results.all_passed) {
                resultComment += '\n🎉 **All performance assertions passed!**\n';
                resultComment += '✅ This PR maintains M4 performance standards.\n';
              } else {
                resultComment += '\n❌ **Performance regression detected!**\n';
                resultComment += '⚠️ Please optimize before merging.\n';
              }
            } else {
              resultComment += '⚠️ Performance results not found. Check workflow logs.\n';
            }
            
          } catch (error) {
            resultComment += `❌ Error reading performance results: ${error.message}\n`;
          }
          
          resultComment += '\n---\n';
          resultComment += `🤖 Generated by M4 Performance Guard • ${new Date().toISOString()}\n`;
          
          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: resultComment
          });

  # Only run on main branch
  performance-baseline:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r dev-requirements.txt
        pip install -e .
        
    - name: 📊 Generate Performance Baseline
      run: |
        echo "📈 Generating performance baseline for main branch..."
        python scripts/performance/m4_simple_benchmark.py
        
    - name: 💾 Store Baseline Results
      uses: actions/upload-artifact@v4
      with:
        name: performance-baseline-${{ github.sha }}
        path: output/m4_simple_benchmark_*.json
        retention-days: 90 